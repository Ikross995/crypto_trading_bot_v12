#!/usr/bin/env python3
"""
üß† GRU Model Training on REAL Binance Data
==========================================

–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ Binance Futures API
–∏ –æ–±—É—á–∞–µ—Ç GRU –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Ü–µ–Ω—ã.

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- –†–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ Binance Futures
- –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ rate limits (2400 weight/min)
- –ü–∞–≥–∏–Ω–∞—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –±–æ–ª—å—à–∏—Ö –æ–±—ä—ë–º–æ–≤
- –†–∞—Å—á—ë—Ç —Ä–µ–∞–ª—å–Ω—ã—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ GPU —á–µ—Ä–µ–∑ TensorFlow
- –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø–∞—Ä–∞—Ö
"""

import asyncio
import logging
import sys
import time
from pathlib import Path
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Optional, Tuple
import numpy as np
import pandas as pd

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –∫–æ—Ä–Ω—é –ø—Ä–æ–µ–∫—Ç–∞
sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    import tensorflow as tf
    from models.gru_predictor import GRUPricePredictor
except ImportError as e:
    print(f"‚ùå Import error: {e}")
    print("Make sure TensorFlow is installed: pip install tensorflow")
    sys.exit(1)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(name)s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


# ==========================================
# üéÆ GPU CONFIGURATION
# ==========================================

def configure_gpu():
    """
    –ù–∞—Å—Ç—Ä–æ–π–∫–∞ TensorFlow –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU.
    """
    logger.info("üéÆ Configuring GPU...")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö GPU
    gpus = tf.config.list_physical_devices('GPU')

    if gpus:
        try:
            # –†–∞–∑—Ä–µ—à–∞–µ–º –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –≤—ã–¥–µ–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)

            logger.info(f"‚úÖ GPU available: {len(gpus)} device(s)")
            for i, gpu in enumerate(gpus):
                logger.info(f"   GPU {i}: {gpu.name}")

            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º GPU –∫–∞–∫ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            tf.config.set_visible_devices(gpus[0], 'GPU')

        except RuntimeError as e:
            logger.warning(f"‚ö†Ô∏è  GPU configuration error: {e}")
            logger.info("üìä Will use CPU instead")
    else:
        logger.info("üìä No GPU found, using CPU")
        logger.info("üí° To use GPU, install: pip install tensorflow-gpu")


# ==========================================
# üì• BINANCE DATA DOWNLOADER
# ==========================================

class BinanceDataDownloader:
    """
    –ó–∞–≥—Ä—É–∑—á–∏–∫ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ Binance Futures API.

    –ü—Ä–∞–≤–∏–ª—å–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç:
    - Rate limits (2400 request weight/min)
    - –ü–∞–≥–∏–Ω–∞—Ü–∏—é –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä—ë–º–æ–≤
    - –û—à–∏–±–∫–∏ —Å–µ—Ç–∏
    - –í–∞–ª–∏–¥–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö
    """

    BASE_URL = "https://fapi.binance.com"  # Futures API
    MAX_LIMIT = 1500  # –ú–∞–∫—Å–∏–º—É–º —Å–≤–µ—á–µ–π –∑–∞ –∑–∞–ø—Ä–æ—Å
    RATE_LIMIT_WEIGHT = 2400  # Weight limit per minute
    WEIGHT_PER_REQUEST = {
        100: 1,
        500: 2,
        1000: 5,
        1500: 10
    }

    def __init__(self):
        self.request_count = 0
        self.request_weight = 0
        self.last_reset = time.time()

    def _get_request_weight(self, limit: int) -> int:
        """–ü–æ–ª—É—á–∏—Ç—å weight –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç limit"""
        if limit <= 100:
            return 1
        elif limit <= 500:
            return 2
        elif limit <= 1000:
            return 5
        else:
            return 10

    async def _rate_limit_check(self, weight: int):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∫–æ–Ω—Ç—Ä–æ–ª—å rate limit"""
        # –°–±—Ä–æ—Å —Å—á—ë—Ç—á–∏–∫–∞ –∫–∞–∂–¥—É—é –º–∏–Ω—É—Ç—É
        now = time.time()
        if now - self.last_reset >= 60:
            self.request_weight = 0
            self.last_reset = now

        # –ï—Å–ª–∏ –ø—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç - –∂–¥—ë–º
        if self.request_weight + weight > self.RATE_LIMIT_WEIGHT:
            wait_time = 60 - (now - self.last_reset)
            logger.warning(f"‚è±Ô∏è  Rate limit reached, waiting {wait_time:.1f}s...")
            await asyncio.sleep(wait_time + 1)
            self.request_weight = 0
            self.last_reset = time.time()

        self.request_weight += weight

    async def fetch_klines(
        self,
        symbol: str,
        interval: str = "1m",
        start_time: Optional[int] = None,
        end_time: Optional[int] = None,
        limit: int = 1500
    ) -> List[List]:
        """
        –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å–≤–µ—á–∏ –∏–∑ Binance Futures API.

        Args:
            symbol: –¢–æ—Ä–≥–æ–≤–∞—è –ø–∞—Ä–∞ (BTCUSDT)
            interval: –¢–∞–π–º—Ñ—Ä–µ–π–º (1m, 5m, 15m, 30m, 1h, 4h, 1d)
            start_time: Unix timestamp –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö
            end_time: Unix timestamp –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö
            limit: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–≤–µ—á–µ–π (–º–∞–∫—Å 1500)

        Returns:
            List of klines: [timestamp, open, high, low, close, volume, ...]
        """
        import aiohttp

        # Rate limit check
        weight = self._get_request_weight(limit)
        await self._rate_limit_check(weight)

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
        params = {
            "symbol": symbol,
            "interval": interval,
            "limit": limit
        }

        if start_time:
            params["startTime"] = start_time
        if end_time:
            params["endTime"] = end_time

        url = f"{self.BASE_URL}/fapi/v1/klines"

        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
        async with aiohttp.ClientSession() as session:
            try:
                async with session.get(url, params=params, timeout=30) as response:
                    if response.status == 200:
                        data = await response.json()
                        self.request_count += 1
                        return data
                    else:
                        logger.error(f"‚ùå API error {response.status}: {await response.text()}")
                        return []
            except Exception as e:
                logger.error(f"‚ùå Request failed: {e}")
                return []

    async def download_historical_data(
        self,
        symbol: str,
        interval: str = "1m",
        days: int = 365
    ) -> pd.DataFrame:
        """
        –ó–∞–≥—Ä—É–∑–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π.

        Args:
            symbol: –¢–æ—Ä–≥–æ–≤–∞—è –ø–∞—Ä–∞
            interval: –¢–∞–π–º—Ñ—Ä–µ–π–º
            days: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –∏—Å—Ç–æ—Ä–∏–∏

        Returns:
            DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏: timestamp, open, high, low, close, volume
        """
        logger.info(f"üì• Downloading {days} days of {symbol} {interval} data...")

        # –†–∞—Å—á—ë—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–∞–º–æ–∫
        end_time = int(datetime.now(timezone.utc).timestamp() * 1000)
        start_time = int((datetime.now(timezone.utc) - timedelta(days=days)).timestamp() * 1000)

        # –†–∞—Å—á—ë—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–ø—Ä–æ—Å–æ–≤
        interval_ms = self._interval_to_ms(interval)
        total_candles = (end_time - start_time) // interval_ms
        total_requests = (total_candles // self.MAX_LIMIT) + 1

        logger.info(f"üìä Total candles: ~{total_candles:,}")
        logger.info(f"üîÑ Required requests: ~{total_requests}")
        logger.info(f"‚è±Ô∏è  Estimated time: ~{total_requests * 0.5:.0f}s")

        all_klines = []
        current_start = start_time
        request_num = 0

        while current_start < end_time:
            request_num += 1

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ—Ä—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö
            klines = await self.fetch_klines(
                symbol=symbol,
                interval=interval,
                start_time=current_start,
                end_time=end_time,
                limit=self.MAX_LIMIT
            )

            if not klines:
                logger.warning(f"‚ö†Ô∏è  No data received for {symbol}, retrying...")
                await asyncio.sleep(5)
                continue

            all_klines.extend(klines)

            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            if request_num % 10 == 0:
                logger.info(f"   Progress: {len(all_klines):,} candles downloaded ({request_num}/{total_requests} requests)")

            # –°–ª–µ–¥—É—é—â–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –ø–æ—Å–ª–µ–¥–Ω–µ–π —Å–≤–µ—á–∏
            current_start = klines[-1][0] + interval_ms

            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ —á—Ç–æ–±—ã –Ω–µ –ø—Ä–µ–≤—ã—Å–∏—Ç—å rate limit
            await asyncio.sleep(0.1)

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ DataFrame
        df = pd.DataFrame(all_klines, columns=[
            'timestamp', 'open', 'high', 'low', 'close', 'volume',
            'close_time', 'quote_volume', 'trades', 'taker_buy_base',
            'taker_buy_quote', 'ignore'
        ])

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
        df['open'] = df['open'].astype(float)
        df['high'] = df['high'].astype(float)
        df['low'] = df['low'].astype(float)
        df['close'] = df['close'].astype(float)
        df['volume'] = df['volume'].astype(float)

        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']]

        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ timestamp
        df = df.drop_duplicates(subset=['timestamp'], keep='last')
        df = df.sort_values('timestamp').reset_index(drop=True)

        logger.info(f"‚úÖ Downloaded {len(df):,} candles for {symbol}")
        logger.info(f"   Range: {df['timestamp'].iloc[0]} ‚Üí {df['timestamp'].iloc[-1]}")
        logger.info(f"   Price: ${df['close'].iloc[0]:.2f} ‚Üí ${df['close'].iloc[-1]:.2f}")

        return df

    @staticmethod
    def _interval_to_ms(interval: str) -> int:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –∏–Ω—Ç–µ—Ä–≤–∞–ª –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã"""
        multipliers = {
            'm': 60 * 1000,
            'h': 60 * 60 * 1000,
            'd': 24 * 60 * 60 * 1000,
            'w': 7 * 24 * 60 * 60 * 1000
        }

        unit = interval[-1]
        value = int(interval[:-1])

        return value * multipliers.get(unit, 60 * 1000)


# ==========================================
# üìä TECHNICAL INDICATORS
# ==========================================

def calculate_technical_indicators(df: pd.DataFrame) -> pd.DataFrame:
    """
    –†–∞—Å—Å—á–∏—Ç–∞—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.

    –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã:
    - RSI (14)
    - MACD (12, 26, 9)
    - Bollinger Bands (20, 2)
    - SMA (20, 50)
    - EMA (50)
    - Volume SMA (20)
    - ATR (14)
    """
    logger.info("üìä Calculating technical indicators...")

    df = df.copy()

    # RSI (14)
    delta = df['close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    df['rsi'] = 100 - (100 / (1 + rs))

    # MACD (12, 26, 9)
    ema_12 = df['close'].ewm(span=12, adjust=False).mean()
    ema_26 = df['close'].ewm(span=26, adjust=False).mean()
    df['macd'] = ema_12 - ema_26
    df['macd_signal'] = df['macd'].ewm(span=9, adjust=False).mean()

    # Bollinger Bands (20, 2)
    sma_20 = df['close'].rolling(window=20).mean()
    std_20 = df['close'].rolling(window=20).std()
    df['bb_upper'] = sma_20 + (std_20 * 2)
    df['bb_lower'] = sma_20 - (std_20 * 2)
    df['bb_mid'] = sma_20

    # SMA
    df['sma_20'] = df['close'].rolling(window=20).mean()
    df['sma_50'] = df['close'].rolling(window=50).mean()

    # EMA
    df['ema_50'] = df['close'].ewm(span=50, adjust=False).mean()

    # Volume indicators
    df['volume_sma'] = df['volume'].rolling(window=20).mean()

    # ATR (14) - Average True Range
    high_low = df['high'] - df['low']
    high_close = np.abs(df['high'] - df['close'].shift())
    low_close = np.abs(df['low'] - df['close'].shift())
    ranges = pd.concat([high_low, high_close, low_close], axis=1)
    true_range = np.max(ranges, axis=1)
    df['atr'] = true_range.rolling(14).mean()

    # –£–¥–∞–ª—è–µ–º NaN
    df = df.dropna()

    logger.info(f"‚úÖ Indicators calculated, {len(df):,} samples remaining")

    return df


# ==========================================
# üéì TRAINING PIPELINE
# ==========================================

async def train_gru_on_real_data(
    symbols: List[str] = None,
    days: int = 365,
    interval: str = "1m",
    save_path: str = "models/checkpoints/gru_model_real.keras"
):
    """
    –û–±—É—á–∏—Ç—å GRU –º–æ–¥–µ–ª—å –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö Binance.

    Args:
        symbols: –°–ø–∏—Å–æ–∫ —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø–∞—Ä (–µ—Å–ª–∏ None - –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è default)
        days: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –∏—Å—Ç–æ—Ä–∏–∏
        interval: –¢–∞–π–º—Ñ—Ä–µ–π–º
        save_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
    """
    # Default symbols
    if symbols is None:
        symbols = [
            'BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'SOLUSDT',
            'ADAUSDT', 'XRPUSDT', 'DOGEUSDT', 'AVAXUSDT',
            'LINKUSDT', 'MATICUSDT'
        ]

    logger.info("=" * 70)
    logger.info("üöÄ GRU Model Training on REAL Binance Data")
    logger.info("=" * 70)
    logger.info(f"üìä Symbols: {', '.join(symbols)}")
    logger.info(f"üìÖ History: {days} days")
    logger.info(f"‚è∞ Interval: {interval}")
    logger.info("=" * 70)

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ GPU
    configure_gpu()

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≥—Ä—É–∑—á–∏–∫–∞
    downloader = BinanceDataDownloader()

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä
    all_data = []

    for i, symbol in enumerate(symbols, 1):
        logger.info("")
        logger.info(f"üì• [{i}/{len(symbols)}] Processing {symbol}...")

        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            df = await downloader.download_historical_data(
                symbol=symbol,
                interval=interval,
                days=days
            )

            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            df = calculate_technical_indicators(df)

            # –î–æ–±–∞–≤–ª—è–µ–º –∫ –æ–±—â–µ–º—É –Ω–∞–±–æ—Ä—É
            all_data.append(df)

            logger.info(f"‚úÖ {symbol}: {len(df):,} samples ready")

        except Exception as e:
            logger.error(f"‚ùå Failed to process {symbol}: {e}")
            continue

    if not all_data:
        logger.error("‚ùå No data collected! Exiting...")
        return None

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
    logger.info("")
    logger.info("üîó Combining data from all symbols...")
    combined_df = pd.concat(all_data, ignore_index=True)
    logger.info(f"‚úÖ Total samples: {len(combined_df):,}")

    # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    feature_columns = [
        'open', 'high', 'low', 'volume',
        'rsi', 'macd', 'macd_signal',
        'bb_upper', 'bb_mid', 'bb_lower',
        'sma_20', 'sma_50', 'ema_50',
        'volume_sma', 'atr'
    ]

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    missing_features = [col for col in feature_columns if col not in combined_df.columns]
    if missing_features:
        logger.error(f"‚ùå Missing features: {missing_features}")
        return None

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    df_features = combined_df[feature_columns + ['close']]

    logger.info("")
    logger.info("üß† Initializing GRU model...")
    logger.info(f"   Features: {len(feature_columns)}")
    logger.info(f"   Sequence length: 60")
    logger.info(f"   GRU units: 100")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    predictor = GRUPricePredictor(
        sequence_length=60,
        features=len(feature_columns),
        gru_units=100,
        dropout_rate=0.2,
        learning_rate=0.001  # –ú–µ–Ω—å—à–∏–π LR –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    )

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    logger.info("")
    logger.info("üìä Preparing training/test split...")
    X_train, y_train, X_test, y_test = predictor.prepare_data(
        data=df_features,
        target_column='close',
        train_split=0.8
    )

    logger.info(f"   Training set: {len(X_train):,} samples")
    logger.info(f"   Test set: {len(X_test):,} samples")

    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    logger.info("")
    logger.info("üöÄ Starting training...")
    logger.info("=" * 70)

    history = await predictor.train(
        X_train=X_train,
        y_train=y_train,
        X_val=X_test,
        y_val=y_test,
        epochs=20,
        batch_size=32,
        verbose=1
    )

    # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
    logger.info("")
    logger.info("üìä Evaluating model on test set...")
    metrics = await predictor.evaluate(X_test, y_test)

    logger.info("=" * 70)
    logger.info("üìà Final Results:")
    logger.info(f"   MAPE: {metrics.get('mape', 0):.2f}%")
    logger.info(f"   RMSE: {metrics.get('rmse', 0):.2f}")
    logger.info(f"   MAE: {metrics.get('mae', 0):.2f}")
    logger.info(f"   R¬≤: {metrics.get('r2', 0):.4f}")
    logger.info("=" * 70)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    logger.info("")
    logger.info(f"üíæ Saving model to {save_path}...")
    predictor.save(save_path)

    logger.info("")
    logger.info("‚úÖ Training completed successfully!")
    logger.info("")
    logger.info("üìù Next steps:")
    logger.info("   1. Set GRU_ENABLE=true in .env")
    logger.info("   2. Run the bot: python cli.py live --timeframe 30m --testnet")
    logger.info("   3. Look for: [PHASE 2] GRU price predictor initialized")

    return predictor, metrics


# ==========================================
# üéØ MAIN
# ==========================================

async def main():
    """Main entry point"""

    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–∑ .env –∏–ª–∏ default
    symbols = [
        'BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'SOLUSDT',
        'ADAUSDT', 'XRPUSDT', 'DOGEUSDT', 'AVAXUSDT',
        'LINKUSDT', 'MATICUSDT'
    ]

    try:
        await train_gru_on_real_data(
            symbols=symbols,
            days=365,  # 1 –≥–æ–¥ –¥–∞–Ω–Ω—ã—Ö
            interval="1m",
            save_path="models/checkpoints/gru_model_real.keras"
        )
    except KeyboardInterrupt:
        logger.info("\n\n‚ö†Ô∏è  Training interrupted by user")
    except Exception as e:
        logger.error(f"\n\n‚ùå Training failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
