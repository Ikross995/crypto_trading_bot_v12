#!/usr/bin/env python3
"""
üîÑ Fine-tune GRU Model on Fresh Data
====================================

–î–æ—Ç—Ä–µ–Ω–∏—Ä–æ–≤—ã–≤–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é GRU –º–æ–¥–µ–ª—å –Ω–∞ —Å–≤–µ–∂–∏—Ö –¥–∞–Ω–Ω—ã—Ö.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    # –î–æ—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 30 –¥–Ω—è—Ö (5 —ç–ø–æ—Ö)
    python scripts/finetune_gru.py --days 30 --epochs 5

    # –î–æ—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –ø–∞—Ä–µ
    python scripts/finetune_gru.py --symbols BTCUSDT --days 60 --epochs 10

    # –î–æ—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞—Ç—å —Å –¥—Ä—É–≥–∏–º learning rate
    python scripts/finetune_gru.py --days 30 --lr 0.0001

–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
- –ê–¥–∞–ø—Ç–∞—Ü–∏—è –∫ –Ω–æ–≤—ã–º —Ä—ã–Ω–æ—á–Ω—ã–º —É—Å–ª–æ–≤–∏—è–º
- –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ (5-10 –º–∏–Ω—É—Ç)
- –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —É–∂–µ –≤—ã—É—á–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
"""

import asyncio
import argparse
import logging
import sys
import time
from pathlib import Path
from datetime import datetime, timezone, timedelta
from typing import List, Optional
import numpy as np
import pandas as pd

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –∫–æ—Ä–Ω—é –ø—Ä–æ–µ–∫—Ç–∞
sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import DataLoader
except ImportError as e:
    print(f"‚ùå Import error: {e}")
    sys.exit(1)

# –ò–º–ø–æ—Ä—Ç—ã –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞
from examples.gru_training_pytorch import (
    BinanceDataDownloader,
    calculate_technical_indicators,
    GRUPriceModel,
    PriceDataset,
    prepare_sequences,
    configure_gpu
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


def load_existing_model(model_path: str, device: torch.device):
    """
    –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å.

    Args:
        model_path: –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        device: –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (cuda/cpu)

    Returns:
        model: –ó–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    """
    logger.info(f"üìÇ Loading existing model from: {model_path}")

    if not Path(model_path).exists():
        raise FileNotFoundError(f"Model not found: {model_path}")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º checkpoint
    checkpoint = torch.load(model_path, map_location=device)

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config = checkpoint['model_config']

    # –°–æ–∑–¥–∞—ë–º –º–æ–¥–µ–ª—å —Å —Ç–æ–π –∂–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π
    model = GRUPriceModel(
        input_features=config['input_features'],
        sequence_length=config['sequence_length']
    ).to(device)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
    model.load_state_dict(checkpoint['model_state_dict'])

    logger.info("‚úÖ Model loaded successfully")
    logger.info(f"   Input features: {config['input_features']}")
    logger.info(f"   Sequence length: {config['sequence_length']}")

    if 'final_metrics' in checkpoint:
        metrics = checkpoint['final_metrics']
        logger.info(f"   Previous MAPE: {metrics.get('mape', 0):.2f}%")

    return model, config


def train_epoch(
    model: nn.Module,
    train_loader: DataLoader,
    criterion: nn.Module,
    optimizer: optim.Optimizer,
    device: torch.device
) -> tuple:
    """–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–µ"""
    model.train()
    train_losses = []
    train_maes = []

    for batch_X, batch_y in train_loader:
        batch_X = batch_X.to(device)
        batch_y = batch_y.to(device)

        # Forward pass
        optimizer.zero_grad()
        predictions = model(batch_X)
        loss = criterion(predictions.squeeze(), batch_y)

        # Backward pass
        loss.backward()
        optimizer.step()

        # –ú–µ—Ç—Ä–∏–∫–∏
        train_losses.append(loss.item())
        mae = torch.mean(torch.abs(predictions.squeeze() - batch_y)).item()
        train_maes.append(mae)

    return np.mean(train_losses), np.mean(train_maes)


def validate_epoch(
    model: nn.Module,
    val_loader: DataLoader,
    criterion: nn.Module,
    device: torch.device
) -> tuple:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è"""
    model.eval()
    val_losses = []
    val_maes = []

    with torch.no_grad():
        for batch_X, batch_y in val_loader:
            batch_X = batch_X.to(device)
            batch_y = batch_y.to(device)

            predictions = model(batch_X)
            loss = criterion(predictions.squeeze(), batch_y)

            val_losses.append(loss.item())
            mae = torch.mean(torch.abs(predictions.squeeze() - batch_y)).item()
            val_maes.append(mae)

    return np.mean(val_losses), np.mean(val_maes)


async def finetune_model(
    model_path: str = "models/checkpoints/gru_model_pytorch.pt",
    symbols: List[str] = None,
    days: int = 30,
    interval: str = "1m",
    epochs: int = 5,
    batch_size: int = 32,
    learning_rate: float = 0.0001,
    save_path: Optional[str] = None
):
    """
    –î–æ—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –º–æ–¥–µ–ª—å –Ω–∞ —Å–≤–µ–∂–∏—Ö –¥–∞–Ω–Ω—ã—Ö.

    Args:
        model_path: –ü—É—Ç—å –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –º–æ–¥–µ–ª–∏
        symbols: –°–ø–∏—Å–æ–∫ –ø–∞—Ä –¥–ª—è –¥–æ—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
        days: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π —Å–≤–µ–∂–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        interval: –¢–∞–π–º—Ñ—Ä–µ–π–º
        epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –¥–æ—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
        batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        learning_rate: Learning rate (–º–µ–Ω—å—à–µ —á–µ–º –ø—Ä–∏ –ø–µ—Ä–≤–∏—á–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏!)
        save_path: –ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å (–µ—Å–ª–∏ None - –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç –∏—Å—Ö–æ–¥–Ω—É—é)
    """
    logger.info("=" * 80)
    logger.info("üîÑ Fine-tuning GRU Model on Fresh Data")
    logger.info("=" * 80)
    logger.info(f"üìã Configuration:")
    logger.info(f"   Existing model: {model_path}")
    logger.info(f"   Fresh data: {days} days")
    logger.info(f"   Symbols: {symbols if symbols else 'Same as original'}")
    logger.info(f"   Epochs: {epochs}")
    logger.info(f"   Learning rate: {learning_rate} (low for fine-tuning)")
    logger.info("=" * 80)

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ GPU
    device = configure_gpu()

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –º–æ–¥–µ–ª—å
    model, config = load_existing_model(model_path, device)

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ –∂–µ —Å–∏–º–≤–æ–ª—ã —á—Ç–æ –∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
    if symbols is None:
        symbols = [
            'BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'SOLUSDT',
            'ADAUSDT', 'XRPUSDT', 'DOGEUSDT', 'AVAXUSDT',
            'LINKUSDT', 'MATICUSDT'
        ]

    # –ó–∞–≥—Ä—É–∑–∫–∞ –°–í–ï–ñ–ò–• –¥–∞–Ω–Ω—ã—Ö
    logger.info(f"üì• Downloading fresh data ({days} days)...")
    downloader = BinanceDataDownloader()
    all_data = []

    for i, symbol in enumerate(symbols, 1):
        logger.info(f"üì• Downloading {symbol} ({i}/{len(symbols)})...")
        df = await downloader.download_historical_data(symbol, interval, days)

        if len(df) > 0:
            df = calculate_technical_indicators(df)
            all_data.append(df)
        else:
            logger.warning(f"‚ö†Ô∏è  Skipping {symbol} - no data")

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
    logger.info("üîó Combining fresh data...")
    combined_df = pd.concat(all_data, ignore_index=True)
    logger.info(f"‚úÖ Fresh dataset: {len(combined_df):,} samples")

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ –∂–µ —Ñ–∏—á–∏ —á—Ç–æ –∏ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
    feature_columns = config['feature_columns']
    sequence_length = config['sequence_length']

    logger.info(f"üìä Using same features: {len(feature_columns)}")

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
    X, y = prepare_sequences(combined_df, feature_columns, sequence_length)

    # Train/Val split (80/20)
    split_idx = int(len(X) * 0.8)
    X_train, X_val = X[:split_idx], X[split_idx:]
    y_train, y_val = y[:split_idx], y[split_idx:]

    logger.info(f"üìä Training samples: {len(X_train):,}")
    logger.info(f"üìä Validation samples: {len(X_val):,}")

    # –°–æ–∑–¥–∞—ë–º DataLoaders
    train_dataset = PriceDataset(X_train, y_train)
    val_dataset = PriceDataset(X_val, y_val)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

    # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    logger.info("üéØ Starting fine-tuning...")
    logger.info(f"   Note: Using LOW learning rate ({learning_rate}) to preserve learned patterns")

    # Fine-tuning
    best_val_loss = float('inf')
    start_time = time.time()

    for epoch in range(epochs):
        # Training
        train_loss, train_mae = train_epoch(model, train_loader, criterion, optimizer, device)

        # Validation
        val_loss, val_mae = validate_epoch(model, val_loader, criterion, device)

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        elapsed = time.time() - start_time
        logger.info(
            f"Epoch {epoch+1}/{epochs} | "
            f"Train Loss: {train_loss:.6f} | "
            f"Val Loss: {val_loss:.6f} | "
            f"Train MAE: {train_mae:.2f} | "
            f"Val MAE: {val_mae:.2f} | "
            f"Time: {elapsed:.1f}s"
        )

        if val_loss < best_val_loss:
            best_val_loss = val_loss
            logger.info(f"   üíæ New best! Val Loss: {best_val_loss:.6f}")

    total_time = time.time() - start_time
    logger.info(f"‚úÖ Fine-tuning completed in {total_time/60:.1f} minutes")

    # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
    logger.info("üìä Final evaluation on validation set...")
    model.eval()
    val_predictions = []
    val_targets = []

    with torch.no_grad():
        for batch_X, batch_y in val_loader:
            batch_X = batch_X.to(device)
            predictions = model(batch_X)
            val_predictions.extend(predictions.cpu().numpy())
            val_targets.extend(batch_y.numpy())

    val_predictions = np.array(val_predictions).flatten()
    val_targets = np.array(val_targets)

    # –ú–µ—Ç—Ä–∏–∫–∏
    mse = np.mean((val_predictions - val_targets) ** 2)
    mae = np.mean(np.abs(val_predictions - val_targets))
    mape = np.mean(np.abs((val_targets - val_predictions) / val_targets)) * 100

    logger.info("=" * 80)
    logger.info("üìä Final metrics after fine-tuning:")
    logger.info(f"   - MSE: {mse:.6f}")
    logger.info(f"   - MAE: {mae:.2f}")
    logger.info(f"   - MAPE: {mape:.2f}%")
    logger.info("=" * 80)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    if save_path is None:
        save_path = model_path

    save_dir = Path(save_path).parent
    save_dir.mkdir(parents=True, exist_ok=True)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å –æ–±–Ω–æ–≤–ª—ë–Ω–Ω–æ–π –∏—Å—Ç–æ—Ä–∏–µ–π
    torch.save({
        'model_state_dict': model.state_dict(),
        'model_config': config,
        'finetuning_info': {
            'finetuned_on': datetime.now().isoformat(),
            'fresh_data_days': days,
            'finetune_epochs': epochs,
            'learning_rate': learning_rate
        },
        'final_metrics': {
            'mse': mse,
            'mae': mae,
            'mape': mape
        }
    }, save_path)

    logger.info(f"‚úÖ Fine-tuned model saved to: {save_path}")
    logger.info("=" * 80)
    logger.info("üéâ Fine-tuning completed successfully!")
    logger.info("=" * 80)
    logger.info("")
    logger.info("üìã Model is now updated with fresh market data!")
    logger.info("   You can continue using it in your bot without any changes.")


def main():
    parser = argparse.ArgumentParser(description="Fine-tune GRU model on fresh data")
    parser.add_argument('--model', type=str, default='models/checkpoints/gru_model_pytorch.pt',
                        help='Path to existing model')
    parser.add_argument('--days', type=int, default=30,
                        help='Days of fresh data to train on (default: 30)')
    parser.add_argument('--epochs', type=int, default=5,
                        help='Number of fine-tuning epochs (default: 5)')
    parser.add_argument('--symbols', type=str, nargs='+',
                        help='Symbols to train on (default: same as original)')
    parser.add_argument('--interval', type=str, default='1m',
                        help='Timeframe (default: 1m)')
    parser.add_argument('--batch-size', type=int, default=32,
                        help='Batch size (default: 32)')
    parser.add_argument('--lr', type=float, default=0.0001,
                        help='Learning rate (default: 0.0001 - low for fine-tuning)')
    parser.add_argument('--save-as', type=str,
                        help='Save to different path (default: overwrite original)')

    args = parser.parse_args()

    asyncio.run(finetune_model(
        model_path=args.model,
        symbols=args.symbols,
        days=args.days,
        interval=args.interval,
        epochs=args.epochs,
        batch_size=args.batch_size,
        learning_rate=args.lr,
        save_path=args.save_as
    ))


if __name__ == "__main__":
    main()
