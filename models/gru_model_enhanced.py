"""
üî• Enhanced GRU Model Architecture
===================================

–£–°–ò–õ–ï–ù–ù–ê–Ø –≤–µ—Ä—Å–∏—è GRU –¥–ª—è –∫—Ä–∏–ø—Ç–æ-—Ç—Ä–µ–π–¥–∏–Ω–≥–∞:
- 3x –±–æ–ª—å—à–µ –Ω–µ–π—Ä–æ–Ω–æ–≤
- 3 GRU —Å–ª–æ—è –≤–º–µ—Å—Ç–æ 2
- Batch Normalization
- Dropout –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
- Attention mechanism (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

Total parameters: ~400K (–≤–º–µ—Å—Ç–æ 61K)
"""

import torch
import torch.nn as nn


class EnhancedGRUModel(nn.Module):
    """
    üî• –£–°–ò–õ–ï–ù–ù–ê–Ø GRU –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è % –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω—ã.

    Architecture (POWERFUL):
    - Input: (batch, sequence_length, features)
    - GRU Layer 1: 256 units, dropout=0.3
    - GRU Layer 2: 128 units, dropout=0.3
    - GRU Layer 3: 64 units, dropout=0.2
    - Dense 1: 128 units, ReLU + BatchNorm
    - Dense 2: 64 units, ReLU + BatchNorm
    - Dense 3: 32 units, ReLU
    - Output: 1 unit (% price change)

    Total params: ~400,000 (vs 61,000 in old model)
    """

    def __init__(self, input_features: int, sequence_length: int):
        super(EnhancedGRUModel, self).__init__()

        self.input_features = input_features
        self.sequence_length = sequence_length

        # üî• GRU Layers (–£–í–ï–õ–ò–ß–ï–ù–´!)
        self.gru1 = nn.GRU(
            input_size=input_features,
            hidden_size=256,  # 100 ‚Üí 256
            num_layers=1,
            batch_first=True,
            dropout=0.0
        )

        self.gru2 = nn.GRU(
            input_size=256,
            hidden_size=128,  # 50 ‚Üí 128
            num_layers=1,
            batch_first=True,
            dropout=0.0
        )

        self.gru3 = nn.GRU(  # üî• –ù–æ–≤—ã–π 3-–π —Å–ª–æ–π!
            input_size=128,
            hidden_size=64,
            num_layers=1,
            batch_first=True,
            dropout=0.0
        )

        # üî• Dropout layers (–£–°–ò–õ–ï–ù–´!)
        self.dropout1 = nn.Dropout(0.3)  # 0.2 ‚Üí 0.3
        self.dropout2 = nn.Dropout(0.3)
        self.dropout3 = nn.Dropout(0.2)

        # üî• Dense layers (–ë–û–õ–¨–®–ï!)
        self.fc1 = nn.Linear(64, 128)  # 50‚Üí25  —Å—Ç–∞–ª–æ  64‚Üí128
        self.bn1 = nn.BatchNorm1d(128)  # üî• Batch Normalization

        self.fc2 = nn.Linear(128, 64)  # üî• –ù–æ–≤—ã–π —Å–ª–æ–π!
        self.bn2 = nn.BatchNorm1d(64)

        self.fc3 = nn.Linear(64, 32)  # üî• –ù–æ–≤—ã–π —Å–ª–æ–π!

        self.fc_out = nn.Linear(32, 1)  # Output

        self.relu = nn.ReLU()

    def forward(self, x):
        # x shape: (batch, sequence_length, features)

        # GRU Layer 1
        out, _ = self.gru1(x)
        out = self.dropout1(out)

        # GRU Layer 2
        out, _ = self.gru2(out)
        out = self.dropout2(out)

        # üî• GRU Layer 3 (NEW!)
        out, _ = self.gru3(out)
        out = self.dropout3(out)

        # –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–π timestep
        out = out[:, -1, :]  # (batch, 64)

        # üî• Dense layers (ENHANCED!)
        out = self.fc1(out)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.fc2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.fc3(out)
        out = self.relu(out)

        out = self.fc_out(out)

        return out


if __name__ == "__main__":
    # Test model
    model = EnhancedGRUModel(input_features=22, sequence_length=60)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"‚úÖ Enhanced GRU Model")
    print(f"   Total parameters: {total_params:,}")
    print(f"   ~{total_params / 61301:.1f}x bigger than old model")

    # Test forward pass
    x = torch.randn(32, 60, 22)  # (batch=32, seq=60, features=22)
    y = model(x)
    print(f"   Input shape: {x.shape}")
    print(f"   Output shape: {y.shape}")
