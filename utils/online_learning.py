"""
üß† Online Learning System for GRU Model
=======================================

–°–∏—Å—Ç–µ–º–∞ –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —É—á–∏—Ç—å—Å—è –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å–¥–µ–ª–∫–∞—Ö.

–ü—Ä–∏–Ω—Ü–∏–ø —Ä–∞–±–æ—Ç—ã:
    1. –ë–æ—Ç –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç —Å–¥–µ–ª–∫—É —Å –ø—Ä–æ–≥–Ω–æ–∑–æ–º GRU
    2. –°–¥–µ–ª–∫–∞ –∑–∞–∫—Ä—ã–≤–∞–µ—Ç—Å—è
    3. –ú–æ–¥–µ–ª—å –ø–æ–ª—É—á–∞–µ—Ç —Ñ–∏–¥–±–µ–∫: –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ç–æ—á–Ω—ã–º –±—ã–ª –ø—Ä–æ–≥–Ω–æ–∑
    4. –ú–æ–¥–µ–ª—å –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç –≤–µ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—à–∏–±–∫–∏

–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
    - –ê–¥–∞–ø—Ç–∞—Ü–∏—è –∫ —Ä–µ–∞–ª—å–Ω—ã–º —É—Å–ª–æ–≤–∏—è–º –≤–∞—à–µ–≥–æ —Ç—Ä–µ–π–¥–∏–Ω–≥–∞
    - –£—á—ë—Ç –≤–∞—à–µ–≥–æ —Å—Ç–∏–ª—è –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
    - –ü–æ—Å—Ç–æ—è–Ω–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –±–µ–∑ —Ä—É—á–Ω–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è

–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤ .env:
    ONLINE_LEARNING_ENABLE=true
    ONLINE_LEARNING_LR=0.00001  # –û—á–µ–Ω—å –Ω–∏–∑–∫–∏–π LR –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
    ONLINE_LEARNING_MIN_TRADES=10  # –ú–∏–Ω–∏–º—É–º —Å–¥–µ–ª–æ–∫ –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
    ONLINE_LEARNING_SAVE_INTERVAL=50  # –°–æ—Ö—Ä–∞–Ω—è—Ç—å –∫–∞–∂–¥—ã–µ N —Å–¥–µ–ª–æ–∫
"""

import logging
import asyncio
from pathlib import Path
from typing import Dict, List, Optional, Any
from datetime import datetime
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque

logger = logging.getLogger(__name__)


class OnlineLearner:
    """
    –°–∏—Å—Ç–µ–º–∞ –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏—è –¥–ª—è GRU –º–æ–¥–µ–ª–∏.

    –ú–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –Ω–∞ –∫–∞–∂–¥–æ–π –∑–∞–∫—Ä—ã—Ç–æ–π —Å–¥–µ–ª–∫–µ, –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ
    –∞–¥–∞–ø—Ç–∏—Ä—É—è—Å—å –∫ —Ä–µ–∞–ª—å–Ω—ã–º —Ä—ã–Ω–æ—á–Ω—ã–º —É—Å–ª–æ–≤–∏—è–º.
    """

    def __init__(
        self,
        model: nn.Module,
        device: torch.device,
        learning_rate: float = 0.00001,
        buffer_size: int = 100,
        min_trades_to_save: int = 10,
        save_interval: int = 50,
        model_save_path: Optional[str] = None
    ):
        """
        Args:
            model: GRU –º–æ–¥–µ–ª—å
            device: cuda –∏–ª–∏ cpu
            learning_rate: LR –¥–ª—è –æ–Ω–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è (–û–ß–ï–ù–¨ –Ω–∏–∑–∫–∏–π!)
            buffer_size: –†–∞–∑–º–µ—Ä –±—É—Ñ–µ—Ä–∞ –æ–ø—ã—Ç–∞
            min_trades_to_save: –ú–∏–Ω–∏–º—É–º —Å–¥–µ–ª–æ–∫ –ø–µ—Ä–µ–¥ –ø–µ—Ä–≤—ã–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
            save_interval: –°–æ—Ö—Ä–∞–Ω—è—Ç—å –∫–∞–∂–¥—ã–µ N —Å–¥–µ–ª–æ–∫
            model_save_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
        """
        self.model = model
        self.device = device
        self.learning_rate = learning_rate

        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Å –æ—á–µ–Ω—å –Ω–∏–∑–∫–∏–º LR –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        self.criterion = nn.MSELoss()

        # Experience replay buffer
        self.buffer_size = buffer_size
        self.experience_buffer = deque(maxlen=buffer_size)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.total_updates = 0
        self.min_trades_to_save = min_trades_to_save
        self.save_interval = save_interval
        self.model_save_path = model_save_path

        # –ú–µ—Ç—Ä–∏–∫–∏
        self.recent_losses = deque(maxlen=50)
        self.recent_maes = deque(maxlen=50)

        logger.info("üß† Online Learning System initialized")
        logger.info(f"   Learning rate: {learning_rate} (very low for stability)")
        logger.info(f"   Buffer size: {buffer_size} trades")
        logger.info(f"   Save interval: {save_interval} trades")

    def add_experience(
        self,
        input_sequence: np.ndarray,
        predicted_price: float,
        actual_price: float,
        profit: float,
        trade_info: Optional[Dict] = None
    ):
        """
        –î–æ–±–∞–≤–∏—Ç—å –æ–ø—ã—Ç –∏–∑ –∑–∞–∫—Ä—ã—Ç–æ–π —Å–¥–µ–ª–∫–∏.

        Args:
            input_sequence: –í—Ö–æ–¥–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å (60, features)
            predicted_price: –ß—Ç–æ –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–ª–∞
            actual_price: –†–µ–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞
            profit: –ü—Ä–∏–±—ã–ª—å/—É–±—ã—Ç–æ–∫ –æ—Ç —Å–¥–µ–ª–∫–∏
            trade_info: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–¥–µ–ª–∫–µ
        """
        experience = {
            'input_sequence': input_sequence,
            'predicted_price': predicted_price,
            'actual_price': actual_price,
            'profit': profit,
            'timestamp': datetime.now(),
            'trade_info': trade_info or {}
        }

        self.experience_buffer.append(experience)

        logger.debug(
            f"üìù Experience added: "
            f"Pred={predicted_price:.2f}, "
            f"Actual={actual_price:.2f}, "
            f"Error={abs(predicted_price - actual_price):.2f}, "
            f"Profit=${profit:.2f}"
        )

    async def learn_from_experience(self, batch_size: int = 32) -> Dict[str, float]:
        """
        –û–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–æ–º –æ–ø—ã—Ç–µ.

        Args:
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è

        Returns:
            –ú–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
        """
        if len(self.experience_buffer) < batch_size:
            logger.debug(f"‚è∏Ô∏è  Not enough experience yet: {len(self.experience_buffer)}/{batch_size}")
            return {}

        # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π –±–∞—Ç—á –∏–∑ –±—É—Ñ–µ—Ä–∞
        import random
        batch = random.sample(list(self.experience_buffer), batch_size)

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X_batch = []
        y_batch = []

        for exp in batch:
            X_batch.append(exp['input_sequence'])
            y_batch.append(exp['actual_price'])

        X_batch = torch.FloatTensor(np.array(X_batch)).to(self.device)
        y_batch = torch.FloatTensor(y_batch).to(self.device)

        # Forward pass
        self.model.train()
        self.optimizer.zero_grad()

        predictions = self.model(X_batch)
        loss = self.criterion(predictions.squeeze(), y_batch)

        # Backward pass
        loss.backward()
        self.optimizer.step()

        # –ú–µ—Ç—Ä–∏–∫–∏
        mae = torch.mean(torch.abs(predictions.squeeze() - y_batch)).item()

        self.recent_losses.append(loss.item())
        self.recent_maes.append(mae)
        self.total_updates += 1

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        logger.info(
            f"üéì Online learning update #{self.total_updates}: "
            f"Loss={loss.item():.6f}, MAE={mae:.2f}, "
            f"Batch size={batch_size}"
        )

        # –ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        if (self.total_updates >= self.min_trades_to_save and
            self.total_updates % self.save_interval == 0):
            await self.save_model()

        return {
            'loss': loss.item(),
            'mae': mae,
            'updates': self.total_updates
        }

    async def save_model(self):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
        if not self.model_save_path:
            logger.warning("‚ö†Ô∏è  Model save path not set, skipping save")
            return

        try:
            # –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            save_path = Path(self.model_save_path)
            save_path.parent.mkdir(parents=True, exist_ok=True)

            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π checkpoint —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å config
            if save_path.exists():
                checkpoint = torch.load(save_path, map_location=self.device)
            else:
                checkpoint = {}

            # –û–±–Ω–æ–≤–ª—è–µ–º
            checkpoint['model_state_dict'] = self.model.state_dict()
            checkpoint['online_learning_info'] = {
                'total_updates': self.total_updates,
                'last_update': datetime.now().isoformat(),
                'avg_recent_loss': np.mean(self.recent_losses) if self.recent_losses else 0,
                'avg_recent_mae': np.mean(self.recent_maes) if self.recent_maes else 0,
                'buffer_size': len(self.experience_buffer)
            }

            torch.save(checkpoint, save_path)

            logger.info(
                f"üíæ Model auto-saved after {self.total_updates} online updates "
                f"(Avg MAE: {np.mean(self.recent_maes):.2f})"
            )

        except Exception as e:
            logger.error(f"‚ùå Failed to save model: {e}")

    def get_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏—è"""
        return {
            'total_updates': self.total_updates,
            'buffer_size': len(self.experience_buffer),
            'avg_recent_loss': np.mean(self.recent_losses) if self.recent_losses else 0,
            'avg_recent_mae': np.mean(self.recent_maes) if self.recent_maes else 0,
            'learning_rate': self.learning_rate
        }


class TradeExperienceCollector:
    """
    –ö–æ–ª–ª–µ–∫—Ç–æ—Ä –æ–ø—ã—Ç–∞ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –±–æ—Ç–æ–º.

    –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ runner/live.py:
        # –ü—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –±–æ—Ç–∞
        self.experience_collector = TradeExperienceCollector(
            online_learner=self.online_learner,
            gru_predictor=self.gru_predictor
        )

        # –ü—Ä–∏ –æ—Ç–∫—Ä—ã—Ç–∏–∏ —Å–¥–µ–ª–∫–∏
        await self.experience_collector.on_trade_opened(
            symbol=symbol,
            entry_price=entry_price,
            side=side,
            input_sequence=input_sequence,
            gru_prediction=gru_prediction
        )

        # –ü—Ä–∏ –∑–∞–∫—Ä—ã—Ç–∏–∏ —Å–¥–µ–ª–∫–∏
        await self.experience_collector.on_trade_closed(
            symbol=symbol,
            exit_price=exit_price,
            pnl=pnl
        )
    """

    def __init__(
        self,
        online_learner: Optional[OnlineLearner] = None,
        gru_predictor: Optional[Any] = None
    ):
        """
        Args:
            online_learner: –°–∏—Å—Ç–µ–º–∞ –æ–Ω–ª–∞–π–Ω-–æ–±—É—á–µ–Ω–∏—è
            gru_predictor: GRU –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å (–¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è input sequence)
        """
        self.online_learner = online_learner
        self.gru_predictor = gru_predictor

        # –ê–∫—Ç–∏–≤–Ω—ã–µ —Å–¥–µ–ª–∫–∏ (–∂–¥—É—Ç –∑–∞–∫—Ä—ã—Ç–∏—è)
        self.open_trades: Dict[str, Dict] = {}

        logger.info("üìä Trade Experience Collector initialized")

    async def on_trade_opened(
        self,
        symbol: str,
        entry_price: float,
        side: str,
        input_sequence: Optional[np.ndarray] = None,
        gru_prediction: Optional[Dict] = None,
        trade_id: Optional[str] = None
    ):
        """
        –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–∫—Ä—ã—Ç–∏–µ —Å–¥–µ–ª–∫–∏.

        Args:
            symbol: –¢–æ—Ä–≥–æ–≤–∞—è –ø–∞—Ä–∞
            entry_price: –¶–µ–Ω–∞ –≤—Ö–æ–¥–∞
            side: LONG/SHORT
            input_sequence: –í—Ö–æ–¥–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è GRU
            gru_prediction: –ü—Ä–æ–≥–Ω–æ–∑ GRU –º–æ–¥–µ–ª–∏
            trade_id: ID —Å–¥–µ–ª–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        if not self.online_learner:
            return

        trade_key = trade_id or f"{symbol}_{entry_price}_{datetime.now().timestamp()}"

        self.open_trades[trade_key] = {
            'symbol': symbol,
            'entry_price': entry_price,
            'side': side,
            'input_sequence': input_sequence,
            'gru_prediction': gru_prediction,
            'opened_at': datetime.now()
        }

        logger.debug(f"üìù Trade opened registered: {symbol} @ ${entry_price} ({side})")

    async def on_trade_closed(
        self,
        symbol: str,
        exit_price: float,
        pnl: float,
        trade_id: Optional[str] = None
    ):
        """
        –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –∑–∞–∫—Ä—ã—Ç–∏–µ —Å–¥–µ–ª–∫–∏ –∏ –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å.

        Args:
            symbol: –¢–æ—Ä–≥–æ–≤–∞—è –ø–∞—Ä–∞
            exit_price: –¶–µ–Ω–∞ –≤—ã—Ö–æ–¥–∞
            pnl: –ü—Ä–∏–±—ã–ª—å/—É–±—ã—Ç–æ–∫
            trade_id: ID —Å–¥–µ–ª–∫–∏
        """
        if not self.online_learner:
            return

        # –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –æ—Ç–∫—Ä—ã—Ç—É—é —Å–¥–µ–ª–∫—É
        trade_key = trade_id
        if trade_key not in self.open_trades:
            # –ò—â–µ–º –ø–æ —Å–∏–º–≤–æ–ª—É (–±–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω—é—é)
            matching_trades = [k for k in self.open_trades.keys() if symbol in k]
            if not matching_trades:
                logger.warning(f"‚ö†Ô∏è  No open trade found for {symbol}")
                return
            trade_key = matching_trades[-1]

        trade_data = self.open_trades.pop(trade_key)

        # –î–æ–±–∞–≤–ª—è–µ–º –æ–ø—ã—Ç
        if trade_data['input_sequence'] is not None:
            predicted_price = (
                trade_data['gru_prediction']['predicted_price']
                if trade_data['gru_prediction']
                else trade_data['entry_price']
            )

            self.online_learner.add_experience(
                input_sequence=trade_data['input_sequence'],
                predicted_price=predicted_price,
                actual_price=exit_price,
                profit=pnl,
                trade_info={
                    'symbol': symbol,
                    'side': trade_data['side'],
                    'entry_price': trade_data['entry_price'],
                    'exit_price': exit_price
                }
            )

            # –û–±—É—á–∞–µ–º—Å—è –Ω–∞ –æ–ø—ã—Ç–µ
            metrics = await self.online_learner.learn_from_experience(batch_size=16)

            if metrics:
                logger.info(
                    f"üéì Learned from trade: {symbol} "
                    f"PnL=${pnl:.2f}, "
                    f"Updates={metrics['updates']}, "
                    f"MAE={metrics['mae']:.2f}"
                )

        logger.debug(f"‚úÖ Trade closed and processed: {symbol} @ ${exit_price}, PnL=${pnl:.2f}")
