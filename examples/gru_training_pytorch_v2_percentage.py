#!/usr/bin/env python3
"""
üß† GRU Model Training v2: PERCENTAGE CHANGE PREDICTION (FIXED!)
================================================================

–ò–ó–ú–ï–ù–ï–ù–ò–ï: –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º % –ò–ó–ú–ï–ù–ï–ù–ò–ï –≤–º–µ—Å—Ç–æ –∞–±—Å–æ–ª—é—Ç–Ω–æ–π —Ü–µ–Ω—ã!

–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
- ‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è –í–°–ï–• –º–æ–Ω–µ—Ç (BTC $103k, DOGE $0.16)
- ‚úÖ –û–¥–∏–Ω scaler –¥–ª—è –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤
- ‚úÖ –ú–æ–¥–µ–ª—å —É—á–∏—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–≤–∏–∂–µ–Ω–∏—è, –Ω–µ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ —Ü–µ–Ω—ã
- ‚úÖ –ù–µ—Ç –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
- ‚úÖ –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç—å

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python examples/gru_training_pytorch_v2_percentage.py --days 180 --epochs 30 --batch-size 1024

BATCH SIZE –¥–ª—è GPU:
- RTX 5070 Ti (16GB): 1024-2048 ‚ö°
- RTX 4090 (24GB): 2048-4096 ‚ö°‚ö°
- RTX 3080 (10GB): 512-1024
- GTX 1080 (8GB): 256-512

–ê–≤—Ç–æ—Ä: Claude + User
–î–∞—Ç–∞: 2025-11-06
"""

import asyncio
import logging
import sys
import time
from pathlib import Path
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Optional, Tuple
import numpy as np
import pandas as pd

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –∫–æ—Ä–Ω—é –ø—Ä–æ–µ–∫—Ç–∞
sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import Dataset, DataLoader
    from sklearn.preprocessing import MinMaxScaler
except ImportError as e:
    print(f"‚ùå Import error: {e}")
    print("Install PyTorch: pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121")
    print("Install scikit-learn: pip install scikit-learn")
    sys.exit(1)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(name)s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


# ==========================================
# üî• IMPORT EXISTING COMPONENTS
# ==========================================

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ –∏–∑ —Å—Ç–∞—Ä–æ–≥–æ —Ñ–∞–π–ª–∞
try:
    # üî• Pass __SKIP_MAIN__ flag to prevent old script's argparse from running
    _old_globals = globals()
    _old_globals['__SKIP_MAIN__'] = True
    exec(open('examples/gru_training_pytorch.py', encoding='utf-8').read(), _old_globals)
    logger.info("‚úÖ Imported existing training components")
except Exception as e:
    logger.error(f"‚ùå Failed to import base training script: {e}")
    logger.error("Make sure examples/gru_training_pytorch.py exists!")
    sys.exit(1)


# ==========================================
# üî• OVERRIDE: PERCENTAGE-BASED SEQUENCES
# ==========================================

def prepare_sequences_percentage(
    df: pd.DataFrame,
    feature_columns: List[str],
    sequence_length: int = 60
) -> Tuple[np.ndarray, np.ndarray, MinMaxScaler, MinMaxScaler, Dict]:
    """
    üî• –ù–û–í–ê–Ø –í–ï–†–°–ò–Ø: –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è GRU.

    –ò–ó–ú–ï–ù–ï–ù–ò–ï: –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º % –ò–ó–ú–ï–ù–ï–ù–ò–ï, –∞ –Ω–µ –∞–±—Å–æ–ª—é—Ç–Ω—É—é —Ü–µ–Ω—É!

    Args:
        df: DataFrame —Å features –∏ close
        feature_columns: –°–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫-—Ñ–∏—á–µ–π
        sequence_length: –î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

    Returns:
        X: (samples, sequence_length, features) - –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ
        y: (samples,) - –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ % –∏–∑–º–µ–Ω–µ–Ω–∏—è
        feature_scaler: Scaler –¥–ª—è features
        target_scaler: Scaler –¥–ª—è % –∏–∑–º–µ–Ω–µ–Ω–∏–π
        stats: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    """
    logger.info(f"üì¶ Preparing sequences with PERCENTAGE CHANGE target (length={sequence_length})...")

    # üî• –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï: –í—ã—á–∏—Å–ª—è–µ–º % –∏–∑–º–µ–Ω–µ–Ω–∏–µ
    logger.info("üî• Computing percentage price changes...")
    df = df.copy()
    df['price_change_pct'] = ((df['close'].shift(-1) - df['close']) / df['close']) * 100

    # –£–¥–∞–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ç—Ä–æ–∫—É (–Ω–µ—Ç —Å–ª–µ–¥—É—é—â–µ–π —Ü–µ–Ω—ã –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞)
    df = df.dropna(subset=['price_change_pct'])

    # üî• –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï: –ö–ª–∏–ø–∞–µ–º —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ % –∏–∑–º–µ–Ω–µ–Ω–∏—è
    # BTC –º–µ–Ω—è–µ—Ç—Å—è –Ω–∞ ¬±1%, DOGE –Ω–∞ ¬±10% ‚Üí –Ω—É–∂–Ω–∞ –æ–¥–∏–Ω–∞–∫–æ–≤–∞—è —à–∫–∞–ª–∞!
    MAX_PCT_CHANGE = 10.0  # –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–π –º–∞–∫—Å–∏–º—É–º –¥–ª—è 30m —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞

    logger.info(f"üìä Price change statistics (BEFORE clipping):")
    logger.info(f"   Min: {df['price_change_pct'].min():.2f}%")
    logger.info(f"   Max: {df['price_change_pct'].max():.2f}%")
    logger.info(f"   Mean: {df['price_change_pct'].mean():.2f}%")
    logger.info(f"   Std: {df['price_change_pct'].std():.2f}%")

    # –ö–ª–∏–ø–∞–µ–º –∫ ¬±10%
    df['price_change_pct'] = df['price_change_pct'].clip(-MAX_PCT_CHANGE, MAX_PCT_CHANGE)

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ % –∏–∑–º–µ–Ω–µ–Ω–∏–π –ü–û–°–õ–ï –∫–ª–∏–ø–∞–Ω–∏—è
    stats = {
        'min_pct': df['price_change_pct'].min(),
        'max_pct': df['price_change_pct'].max(),
        'mean_pct': df['price_change_pct'].mean(),
        'std_pct': df['price_change_pct'].std(),
        'median_pct': df['price_change_pct'].median()
    }

    logger.info(f"üìä Price change statistics (AFTER clipping to ¬±{MAX_PCT_CHANGE}%):")
    logger.info(f"   Min: {stats['min_pct']:.2f}%")
    logger.info(f"   Max: {stats['max_pct']:.2f}%")
    logger.info(f"   Mean: {stats['mean_pct']:.2f}%")
    logger.info(f"   Median: {stats['median_pct']:.2f}%")
    logger.info(f"   Std: {stats['std_pct']:.2f}%")
    logger.info(f"   üî• ALL COINS NOW ON SAME SCALE!")

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è features (0-1)
    logger.info("üîÑ Normalizing features to 0-1 range...")
    feature_scaler = MinMaxScaler()
    features_normalized = feature_scaler.fit_transform(df[feature_columns])

    # üî• –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è % –∏–∑–º–µ–Ω–µ–Ω–∏–π (0-1)
    # NOTE: MinMaxScaler –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞–π–¥–µ—Ç min/max –≤ –¥–∞–Ω–Ω—ã—Ö
    target_scaler = MinMaxScaler()
    target_normalized = target_scaler.fit_transform(df[['price_change_pct']]).flatten()

    logger.info(f"   Feature range: {features_normalized.min():.4f} - {features_normalized.max():.4f}")
    logger.info(f"   Target (% change) range after normalization: {target_normalized.min():.4f} - {target_normalized.max():.4f}")
    logger.info(f"   Target scaler fitted on: {target_scaler.data_min_[0]:.2f}% to {target_scaler.data_max_[0]:.2f}%")

    # –°–æ–∑–¥–∞—ë–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    X, y = [], []

    for i in range(len(df) - sequence_length):
        X.append(features_normalized[i:i + sequence_length])
        y.append(target_normalized[i + sequence_length])

    X = np.array(X)
    y = np.array(y)

    logger.info(f"‚úÖ Sequences prepared:")
    logger.info(f"   X shape: {X.shape}")
    logger.info(f"   y shape: {y.shape}")
    logger.info(f"   üî• Target: PERCENTAGE CHANGE (not absolute price!)")

    return X, y, feature_scaler, target_scaler, stats


# ==========================================
# üî• MAIN TRAINING FUNCTION (OVERRIDDEN)
# ==========================================

async def train_gru_percentage_model(
    symbols: List[str] = None,
    days: int = 180,  # 6 –º–µ—Å—è—Ü–µ–≤ —Å–≤–µ–∂–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    interval: str = "30m",  # 30-–º–∏–Ω—É—Ç–Ω—ã–π —Ç–∞–π–º—Ñ—Ä–µ–π–º
    sequence_length: int = 60,
    epochs: int = 30,
    batch_size: int = 1024,  # üî• –û–ì–†–û–ú–ù–´–ô –¥–ª—è RTX 5070 Ti - MAX SPEED!
    save_path: str = "models/checkpoints/gru_model_pytorch_v2_percentage.pt",
    use_cache: bool = False
):
    """
    üî• –û–±—É—á–∏—Ç—å GRU –º–æ–¥–µ–ª—å –Ω–∞ % –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö (–ü–†–ê–í–ò–õ–¨–ù–û!)

    Args:
        symbols: –°–ø–∏—Å–æ–∫ —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø–∞—Ä
        days: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –∏—Å—Ç–æ—Ä–∏–∏ (180 = 6 –º–µ—Å—è—Ü–µ–≤ –°–í–ï–ñ–ò–• –¥–∞–Ω–Ω—ã—Ö)
        interval: –¢–∞–π–º—Ñ—Ä–µ–π–º (30m –¥–ª—è trading)
        sequence_length: –î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è
        batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (128 –¥–ª—è GPU)
        save_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
    """
    # Default symbols
    if symbols is None:
        symbols = [
            'BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'SOLUSDT',
            'ADAUSDT', 'XRPUSDT', 'DOGEUSDT', 'AVAXUSDT',
            'LINKUSDT', 'APTUSDT'
        ]

    logger.info("=" * 80)
    logger.info("üî• GRU Model Training v2: PERCENTAGE CHANGE PREDICTION")
    logger.info("=" * 80)
    logger.info(f"üìã Configuration:")
    logger.info(f"   Symbols: {', '.join(symbols)}")
    logger.info(f"   Days: {days} (last {days} days of FRESH data)")
    logger.info(f"   Interval: {interval}")
    logger.info(f"   Sequence length: {sequence_length}")
    logger.info(f"   Epochs: {epochs}")
    logger.info(f"   Batch size: {batch_size}")
    logger.info(f"   üî• Target: PERCENTAGE CHANGE (not price!)")
    logger.info("=" * 80)

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ GPU
    device = configure_gpu()

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    downloader = BinanceDataDownloader()
    all_data = []

    for i, symbol in enumerate(symbols, 1):
        logger.info(f"üì• Downloading {symbol} ({i}/{len(symbols)})...")
        df = await downloader.download_historical_data(symbol, interval, days)

        if len(df) > 0:
            df = calculate_technical_indicators(df)
            all_data.append(df)
            logger.info(f"   ‚úÖ {symbol}: {len(df):,} candles")
        else:
            logger.warning(f"‚ö†Ô∏è  Skipping {symbol} - no data")

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
    logger.info("üîó Combining data from all symbols...")
    combined_df = pd.concat(all_data, ignore_index=True)
    logger.info(f"‚úÖ Combined dataset: {len(combined_df):,} samples")

    # –°–ø–∏—Å–æ–∫ —Ñ–∏—á–µ–π (22 indicators)
    feature_columns = [
        # Price features (4)
        'open', 'high', 'low', 'volume',
        # Technical indicators (11)
        'rsi', 'macd', 'macd_signal',
        'bb_upper', 'bb_mid', 'bb_lower',
        'sma_20', 'sma_50', 'ema_50',
        'volume_sma', 'atr',
        # Volume indicators (7)
        'volume_delta', 'obv', 'volume_ratio',
        'volume_spike', 'mfi', 'cvd', 'vwap_distance'
    ]

    logger.info(f"üìä Features: {len(feature_columns)} total (15 price + 7 volume)")

    # üî• –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π (–ù–û–í–ê–Ø –í–ï–†–°–ò–Ø —Å %)
    X, y, feature_scaler, target_scaler, stats = prepare_sequences_percentage(
        combined_df, feature_columns, sequence_length
    )

    # Train/Test split (80/20)
    split_idx = int(len(X) * 0.8)
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]

    logger.info(f"üìä Train samples: {len(X_train):,}")
    logger.info(f"üìä Test samples: {len(X_test):,}")

    # –°–æ–∑–¥–∞—ë–º DataLoaders
    train_dataset = PriceDataset(X_train, y_train)
    test_dataset = PriceDataset(X_test, y_test)

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=0,
        pin_memory=True
    )
    val_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0,
        pin_memory=True
    )

    # –°–æ–∑–¥–∞—ë–º –º–æ–¥–µ–ª—å
    logger.info("üß† Building GRU model...")
    model = GRUPriceModel(
        input_features=len(feature_columns),
        sequence_length=sequence_length
    ).to(device)

    total_params = sum(p.numel() for p in model.parameters())
    logger.info(f"‚úÖ Model parameters: {total_params:,}")

    # –û–±—É—á–µ–Ω–∏–µ
    history = train_model(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        device=device,
        epochs=epochs,
        learning_rate=0.001
    )

    # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
    logger.info("üìä Final evaluation on test set...")
    model.eval()
    test_predictions = []
    test_targets = []

    with torch.no_grad():
        for batch_X, batch_y in val_loader:
            batch_X = batch_X.to(device)
            predictions = model(batch_X)
            test_predictions.extend(predictions.cpu().numpy())
            test_targets.extend(batch_y.numpy())

    test_predictions = np.array(test_predictions).flatten()
    test_targets = np.array(test_targets)

    # üî• –î–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—Ä–∞—Ç–Ω–æ –≤ % –∏–∑–º–µ–Ω–µ–Ω–∏—è
    logger.info("üîÑ Denormalizing predictions back to percentage changes...")
    test_predictions_pct = target_scaler.inverse_transform(test_predictions.reshape(-1, 1)).flatten()
    test_targets_pct = target_scaler.inverse_transform(test_targets.reshape(-1, 1)).flatten()

    # –ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ % –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö
    mae_pct = np.mean(np.abs(test_predictions_pct - test_targets_pct))
    mse_pct = np.mean((test_predictions_pct - test_targets_pct) ** 2)
    rmse_pct = np.sqrt(mse_pct)

    # –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è (—Å–∞–º–∞—è –≤–∞–∂–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞!)
    direction_correct = np.sum(np.sign(test_predictions_pct) == np.sign(test_targets_pct))
    direction_accuracy = direction_correct / len(test_predictions_pct) * 100

    logger.info("=" * 80)
    logger.info("üìä Final metrics (PERCENTAGE CHANGES):")
    logger.info(f"   üî• Direction Accuracy: {direction_accuracy:.2f}%  ‚Üê MOST IMPORTANT!")
    logger.info(f"   - MAE: {mae_pct:.2f}%")
    logger.info(f"   - RMSE: {rmse_pct:.2f}%")
    logger.info(f"   - MSE: {mse_pct:.2f}%¬≤")
    logger.info("=" * 80)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    save_dir = Path(save_path).parent
    save_dir.mkdir(parents=True, exist_ok=True)

    torch.save({
        'model_state_dict': model.state_dict(),
        'model_config': {
            'input_features': len(feature_columns),
            'sequence_length': sequence_length,
            'feature_columns': feature_columns,
            'model_version': 'v2_percentage',  # üî• –ú–µ—Ç–∫–∞ –≤–µ—Ä—Å–∏–∏
            'target_type': 'percentage_change'  # üî• –¢–∏–ø target
        },
        'scalers': {
            'feature_scaler': feature_scaler,
            'target_scaler': target_scaler
        },
        'training_history': history,
        'final_metrics': {
            'mae_pct': mae_pct,
            'rmse_pct': rmse_pct,
            'mse_pct': mse_pct,
            'direction_accuracy': direction_accuracy
        },
        'percentage_stats': stats  # üî• –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ % –∏–∑–º–µ–Ω–µ–Ω–∏–π
    }, save_path)

    logger.info(f"‚úÖ Model saved to: {save_path}")
    logger.info(f"   Model size: {Path(save_path).stat().st_size / 1024 / 1024:.1f} MB")

    logger.info("=" * 80)
    logger.info("üéâ Training completed successfully!")
    logger.info("=" * 80)
    logger.info("")
    logger.info("üìã Next steps:")
    logger.info(f"   1. Update .env: GRU_ENABLE=true")
    logger.info(f"   2. Update .env: GRU_MODEL_PATH={save_path}")
    logger.info(f"   3. Update gru_predictor_pytorch.py to use % predictions")
    logger.info(f"   4. Run bot: python cli.py live --timeframe 30m --use-imba")


# ==========================================
# üöÄ MAIN
# ==========================================

# üî• Only run if NOT being imported by train_gru_final.py
if __name__ == "__main__" and not globals().get('__SKIP_MAIN__'):
    import argparse

    parser = argparse.ArgumentParser(description="Train GRU model on % changes (v2)")
    parser.add_argument('--days', type=int, default=180,
                        help='Days of historical data (default: 180 = 6 months fresh data)')
    parser.add_argument('--interval', type=str, default='30m',
                        help='Timeframe: 1m, 5m, 15m, 30m, 1h, 4h (default: 30m)')
    parser.add_argument('--sequence-length', type=int, default=60,
                        help='Sequence length for LSTM/GRU (default: 60)')
    parser.add_argument('--epochs', type=int, default=30,
                        help='Number of training epochs (default: 30)')
    parser.add_argument('--batch-size', type=int, default=1024,
                        help='Batch size (default: 1024 for RTX 5070 Ti - MAX SPEED!)')
    parser.add_argument('--symbols', type=str, nargs='+',
                        help='Symbols to train on (default: top 10)')

    args = parser.parse_args()

    asyncio.run(train_gru_percentage_model(
        symbols=args.symbols,
        days=args.days,
        interval=args.interval,
        sequence_length=args.sequence_length,
        epochs=args.epochs,
        batch_size=args.batch_size
    ))
