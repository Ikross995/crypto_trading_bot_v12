#!/usr/bin/env python3
"""
üß† META-LEARNER - –ì–ª–∞–≤–Ω—ã–π –º–æ–∑–≥ COMBO —Å–∏—Å—Ç–µ–º—ã!
==============================================

–ú–ê–ö–°–ò–ú–ê–õ–¨–ù–ê–Ø –°–ò–õ–ê:
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –í–°–ï –ø–æ–¥—Ö–æ–¥—ã –≤ –µ–¥–∏–Ω—É—é —Å–∏—Å—Ç–µ–º—É:

1. ü§ñ RL Agent - –£—á–∏—Ç—Å—è —Ç–æ—Ä–≥–æ–≤–∞—Ç—å —á–µ—Ä–µ–∑ –æ–ø—ã—Ç
2. üîÑ Walk-Forward - –ê–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è –∫ —Ä—ã–Ω–∫—É
3. üìä Performance Analyzer - –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —á—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç
4. üéØ Ensemble - –ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª–∏
5. üß† Meta-Model - –£—á–∏—Ç—Å—è –∫–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∂–¥—ã–π –ø–æ–¥—Ö–æ–¥

–°—Ç—Ä–∞—Ç–µ–≥–∏—è:
- –í —Ç—Ä–µ–Ω–¥–æ–≤—ã—Ö —Ä—ã–Ω–∫–∞—Ö ‚Üí RL Agent
- –í –±–æ–∫–æ–≤–∏–∫–∞—Ö ‚Üí Ensemble Conservative
- –í –≤—ã—Å–æ–∫–æ–π –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ ‚Üí Walk-Forward –∞–¥–∞–ø—Ç–∞—Ü–∏—è
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à–∏–π –ø–æ–¥—Ö–æ–¥ –¥–ª—è —Ç–µ–∫—É—â–∏—Ö —É—Å–ª–æ–≤–∏–π

Meta-Learning:
- –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏—Å—Ç–æ—Ä–∏—é
- –£—á–∏—Ç—Å—è –∫–æ–≥–¥–∞ –∫–∞–∫–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç
- –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–∫–ª—é—á–∞–µ—Ç—Å—è
- –ú–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ—Ç Sharpe Ratio

–ê–≤—Ç–æ—Ä: Claude (Anthropic)
"""

import asyncio
import logging
import sys
import json
import time
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, asdict
from collections import deque
import numpy as np
import pandas as pd

sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    import torch
    import torch.nn as nn
except ImportError as e:
    print(f"‚ùå Import error: {e}")
    sys.exit(1)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


# ==========================================
# üìä MARKET REGIME DETECTOR
# ==========================================

class MarketRegimeDetector:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–µ–∫—É—â–∏–π —Ä–µ–∂–∏–º —Ä—ã–Ω–∫–∞

    –†–µ–∂–∏–º—ã:
    - TRENDING_BULL: –°–∏–ª—å–Ω—ã–π —Ä–æ—Å—Ç
    - TRENDING_BEAR: –°–∏–ª—å–Ω–æ–µ –ø–∞–¥–µ–Ω–∏–µ
    - VOLATILE: –í—ã—Å–æ–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
    - SIDEWAYS: –ë–æ–∫–æ–≤–æ–µ –¥–≤–∏–∂–µ–Ω–∏–µ
    - QUIET: –ù–∏–∑–∫–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
    """

    @staticmethod
    def detect_regime(data: pd.DataFrame, lookback: int = 100) -> Dict:
        """
        –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ä–µ–∂–∏–º —Ä—ã–Ω–∫–∞

        Args:
            data: DataFrame —Å OHLCV
            lookback: –ü–µ—Ä–∏–æ–¥ –∞–Ω–∞–ª–∏–∑–∞

        Returns:
            Dict —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ä–µ–∂–∏–º–µ
        """
        recent_data = data.tail(lookback)

        # Calculate metrics
        returns = recent_data['close'].pct_change()
        volatility = returns.std() * 100

        # Trend strength
        first_price = recent_data['close'].iloc[0]
        last_price = recent_data['close'].iloc[-1]
        total_return = (last_price - first_price) / first_price * 100

        # Volume analysis
        avg_volume = recent_data['volume'].mean()
        recent_volume = recent_data['volume'].tail(20).mean()
        volume_ratio = recent_volume / avg_volume if avg_volume > 0 else 1

        # Determine regime
        if abs(total_return) > 10 and volatility < 2:
            if total_return > 0:
                regime = 'TRENDING_BULL'
                confidence = min(abs(total_return) / 20, 1.0)
            else:
                regime = 'TRENDING_BEAR'
                confidence = min(abs(total_return) / 20, 1.0)

        elif volatility > 3:
            regime = 'VOLATILE'
            confidence = min(volatility / 5, 1.0)

        elif abs(total_return) < 3 and volatility < 1.5:
            regime = 'QUIET'
            confidence = 0.7

        else:
            regime = 'SIDEWAYS'
            confidence = 0.6

        return {
            'regime': regime,
            'confidence': confidence,
            'volatility': volatility,
            'trend': total_return,
            'volume_ratio': volume_ratio
        }


# ==========================================
# üéØ STRATEGY SELECTOR
# ==========================================

@dataclass
class StrategyPerformance:
    """–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏"""
    strategy_name: str
    regime: str
    win_rate: float
    sharpe_ratio: float
    total_trades: int
    avg_return: float
    last_used: str


class StrategySelector:
    """
    –í—ã–±–∏—Ä–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –¥–ª—è —Ç–µ–∫—É—â–∏—Ö —É—Å–ª–æ–≤–∏–π

    –°—Ç—Ä–∞—Ç–µ–≥–∏–∏:
    1. rl_agent - Reinforcement Learning
    2. ensemble_conservative - –ö–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π –∞–Ω—Å–∞–º–±–ª—å
    3. ensemble_aggressive - –ê–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –∞–Ω—Å–∞–º–±–ª—å
    4. walk_forward - –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å
    5. best_single - –õ—É—á—à–∞—è –æ–¥–∏–Ω–æ—á–Ω–∞—è –º–æ–¥–µ–ª—å
    """

    def __init__(self):
        self.performance_history: List[StrategyPerformance] = []
        self.regime_preferences: Dict[str, str] = {
            'TRENDING_BULL': 'rl_agent',
            'TRENDING_BEAR': 'ensemble_conservative',
            'VOLATILE': 'walk_forward',
            'SIDEWAYS': 'ensemble_aggressive',
            'QUIET': 'best_single'
        }

    def select_strategy(
        self,
        market_regime: Dict,
        available_strategies: List[str]
    ) -> str:
        """
        –í—ã–±—Ä–∞—Ç—å –ª—É—á—à—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é

        Args:
            market_regime: –†–µ–∑—É–ª—å—Ç–∞—Ç MarketRegimeDetector
            available_strategies: –î–æ—Å—Ç—É–ø–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏

        Returns:
            strategy_name
        """
        regime = market_regime['regime']
        confidence = market_regime['confidence']

        # Check historical performance
        regime_history = [
            p for p in self.performance_history
            if p.regime == regime
        ]

        if regime_history and len(regime_history) >= 5:
            # Use historical data
            best_strategy = max(
                regime_history,
                key=lambda x: x.sharpe_ratio
            )
            strategy = best_strategy.strategy_name

            logger.info(
                f"üéØ Selected {strategy} for {regime} "
                f"(historical Sharpe={best_strategy.sharpe_ratio:.2f})"
            )
        else:
            # Use default preferences
            strategy = self.regime_preferences.get(regime, 'ensemble_conservative')

            logger.info(
                f"üéØ Selected {strategy} for {regime} "
                f"(default preference, confidence={confidence:.1%})"
            )

        return strategy

    def update_performance(
        self,
        strategy_name: str,
        regime: str,
        metrics: Dict
    ):
        """–û–±–Ω–æ–≤–∏—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏"""
        perf = StrategyPerformance(
            strategy_name=strategy_name,
            regime=regime,
            win_rate=metrics.get('win_rate', 0),
            sharpe_ratio=metrics.get('sharpe_ratio', 0),
            total_trades=metrics.get('total_trades', 0),
            avg_return=metrics.get('avg_return', 0),
            last_used=datetime.now().isoformat()
        )

        self.performance_history.append(perf)

        logger.info(
            f"üìä Updated {strategy_name} performance: "
            f"WR={perf.win_rate:.1f}%, Sharpe={perf.sharpe_ratio:.2f}"
        )


# ==========================================
# üß† META-LEARNER
# ==========================================

class MetaLearner:
    """
    –ì–ª–∞–≤–Ω—ã–π –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä COMBO —Å–∏—Å—Ç–µ–º—ã

    –§—É–Ω–∫—Ü–∏–∏:
    1. –î–µ—Ç–µ–∫—Ç —Ä–µ–∂–∏–º–∞ —Ä—ã–Ω–∫–∞
    2. –í—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
    3. –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    4. –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –∏—Å—Ç–æ—Ä–∏–∏
    5. –ê–¥–∞–ø—Ç–∞—Ü–∏—è –∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º
    """

    def __init__(self, config: Dict = None):
        self.config = config or {}

        # Components
        self.regime_detector = MarketRegimeDetector()
        self.strategy_selector = StrategySelector()

        # Loaded models
        self.models = {}

        # History
        self.decision_history = []

        logger.info("üß† Meta-Learner initialized")

    def load_models(
        self,
        rl_agent_path: Optional[str] = None,
        ensemble_path: Optional[str] = None,
        walk_forward_path: Optional[str] = None
    ):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ –º–æ–¥–µ–ª–∏"""
        logger.info("üì• Loading models...")

        # Here you would load actual models
        # For now, just mark as available
        if rl_agent_path:
            self.models['rl_agent'] = 'loaded'
            logger.info(f"   ‚úÖ RL Agent loaded from {rl_agent_path}")

        if ensemble_path:
            self.models['ensemble'] = 'loaded'
            logger.info(f"   ‚úÖ Ensemble loaded from {ensemble_path}")

        if walk_forward_path:
            self.models['walk_forward'] = 'loaded'
            logger.info(f"   ‚úÖ Walk-Forward loaded from {walk_forward_path}")

        logger.info(f"üìä Total models loaded: {len(self.models)}")

    async def predict(
        self,
        data: pd.DataFrame,
        X: np.ndarray
    ) -> Dict:
        """
        –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è

        Args:
            data: OHLCV data –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–∂–∏–º–∞
            X: Feature data –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è

        Returns:
            Dict —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        # 1. Detect market regime
        regime_info = self.regime_detector.detect_regime(data)

        logger.info(f"\nüåç Market Regime: {regime_info['regime']} "
                   f"(confidence={regime_info['confidence']:.1%})")
        logger.info(f"   Volatility: {regime_info['volatility']:.2f}%")
        logger.info(f"   Trend: {regime_info['trend']:+.2f}%")

        # 2. Select strategy
        available_strategies = list(self.models.keys())
        selected_strategy = self.strategy_selector.select_strategy(
            regime_info,
            available_strategies
        )

        # 3. Get prediction from selected strategy
        # (In real implementation, call actual model)
        prediction = self._get_strategy_prediction(selected_strategy, X)

        # 4. Apply confidence weighting
        final_prediction = prediction * regime_info['confidence']

        # 5. Log decision
        decision = {
            'timestamp': datetime.now().isoformat(),
            'regime': regime_info['regime'],
            'strategy': selected_strategy,
            'prediction': float(final_prediction),
            'confidence': regime_info['confidence']
        }
        self.decision_history.append(decision)

        return decision

    def _get_strategy_prediction(
        self,
        strategy: str,
        X: np.ndarray
    ) -> float:
        """Get prediction from strategy"""
        # Placeholder - in real implementation, call actual model
        return np.random.randn() * 0.5

    async def backtest(
        self,
        data: pd.DataFrame,
        window_size: int = 1000,
        step_size: int = 100
    ) -> Dict:
        """
        –ë—ç–∫—Ç–µ—Å—Ç Meta-Learner –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö

        Args:
            data: Historical OHLCV data
            window_size: –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            step_size: –®–∞–≥ –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è –æ–∫–Ω–∞

        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—ç–∫—Ç–µ—Å—Ç–∞
        """
        logger.info("=" * 80)
        logger.info("üß™ META-LEARNER BACKTEST")
        logger.info("=" * 80)
        logger.info(f"Data samples: {len(data):,}")
        logger.info(f"Window size: {window_size}")
        logger.info(f"Step size: {step_size}")
        logger.info("=" * 80)

        results = {
            'trades': [],
            'regime_changes': [],
            'strategy_switches': []
        }

        current_pos = window_size
        balance = 10000
        position = 0

        while current_pos < len(data) - step_size:
            # Get window
            window_data = data.iloc[current_pos-window_size:current_pos]

            # Detect regime
            regime_info = self.regime_detector.detect_regime(window_data)

            # Select strategy
            strategy = self.strategy_selector.select_strategy(
                regime_info,
                ['rl_agent', 'ensemble', 'walk_forward']
            )

            # Simulate trade (placeholder logic)
            future_price = data.iloc[current_pos + step_size]['close']
            current_price = data.iloc[current_pos]['close']
            price_change = (future_price - current_price) / current_price * 100

            # Record
            results['trades'].append({
                'timestamp': data.iloc[current_pos].name,
                'regime': regime_info['regime'],
                'strategy': strategy,
                'price_change': price_change
            })

            # Move window
            current_pos += step_size

            if len(results['trades']) % 10 == 0:
                logger.info(f"   Processed {len(results['trades'])} windows...")

        # Calculate metrics
        trades_df = pd.DataFrame(results['trades'])

        if len(trades_df) > 0:
            total_return = trades_df['price_change'].sum()
            win_rate = (trades_df['price_change'] > 0).sum() / len(trades_df) * 100

            logger.info("\n" + "=" * 80)
            logger.info("üìä BACKTEST RESULTS")
            logger.info("=" * 80)
            logger.info(f"Total trades: {len(trades_df)}")
            logger.info(f"Win Rate: {win_rate:.2f}%")
            logger.info(f"Total Return: {total_return:+.2f}%")
            logger.info("=" * 80)

            # Regime breakdown
            logger.info("\nüìä Performance by Regime:")
            for regime in trades_df['regime'].unique():
                regime_trades = trades_df[trades_df['regime'] == regime]
                regime_wr = (regime_trades['price_change'] > 0).sum() / len(regime_trades) * 100
                regime_return = regime_trades['price_change'].sum()

                logger.info(
                    f"   {regime:15s}: WR={regime_wr:5.1f}%, "
                    f"Return={regime_return:+7.2f}%, "
                    f"Trades={len(regime_trades)}"
                )

            # Strategy breakdown
            logger.info("\nüéØ Performance by Strategy:")
            for strategy in trades_df['strategy'].unique():
                strat_trades = trades_df[trades_df['strategy'] == strategy]
                strat_wr = (strat_trades['price_change'] > 0).sum() / len(strat_trades) * 100
                strat_return = strat_trades['price_change'].sum()

                logger.info(
                    f"   {strategy:20s}: WR={strat_wr:5.1f}%, "
                    f"Return={strat_return:+7.2f}%, "
                    f"Trades={len(strat_trades)}"
                )

        return results

    def save_state(self, path: str):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ Meta-Learner"""
        state = {
            'decision_history': self.decision_history,
            'strategy_performance': [
                asdict(p) for p in self.strategy_selector.performance_history
            ],
            'config': self.config
        }

        with open(path, 'w') as f:
            json.dump(state, f, indent=2)

        logger.info(f"üíæ Meta-Learner state saved to {path}")

    def load_state(self, path: str):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ"""
        with open(path, 'r') as f:
            state = json.load(f)

        self.decision_history = state.get('decision_history', [])
        self.config = state.get('config', {})

        # Restore performance history
        for perf_dict in state.get('strategy_performance', []):
            perf = StrategyPerformance(**perf_dict)
            self.strategy_selector.performance_history.append(perf)

        logger.info(f"‚úÖ Meta-Learner state loaded from {path}")

    def print_summary(self):
        """–í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        logger.info("\n" + "=" * 80)
        logger.info("üß† META-LEARNER SUMMARY")
        logger.info("=" * 80)

        logger.info(f"Models loaded: {len(self.models)}")
        logger.info(f"Decisions made: {len(self.decision_history)}")
        logger.info(f"Performance records: {len(self.strategy_selector.performance_history)}")

        if self.decision_history:
            # Regime distribution
            regimes = [d['regime'] for d in self.decision_history]
            logger.info("\nüìä Regime Distribution:")
            for regime in set(regimes):
                count = regimes.count(regime)
                pct = count / len(regimes) * 100
                logger.info(f"   {regime:15s}: {count:4d} ({pct:5.1f}%)")

            # Strategy usage
            strategies = [d['strategy'] for d in self.decision_history]
            logger.info("\nüéØ Strategy Usage:")
            for strategy in set(strategies):
                count = strategies.count(strategy)
                pct = count / len(strategies) * 100
                logger.info(f"   {strategy:20s}: {count:4d} ({pct:5.1f}%)")

        logger.info("=" * 80)


# ==========================================
# üöÄ MAIN ORCHESTRATOR
# ==========================================

async def run_meta_learner_demo():
    """–î–µ–º–æ Meta-Learner"""

    # Load data
    from gru_training_pytorch import (
        BinanceDataDownloader,
        calculate_technical_indicators
    )

    logger.info("üì• Loading data for demo...")
    downloader = BinanceDataDownloader()
    data = await downloader.download_historical_data('BTCUSDT', '30m', 90)
    data = calculate_technical_indicators(data)

    # Create Meta-Learner
    meta = MetaLearner()

    # "Load" models (placeholder)
    meta.load_models(
        rl_agent_path='models/rl_agent.pt',
        ensemble_path='models/ensemble/',
        walk_forward_path='models/walk_forward.pt'
    )

    # Run backtest
    results = await meta.backtest(data, window_size=500, step_size=50)

    # Print summary
    meta.print_summary()

    # Save state
    meta.save_state('data/meta_learner_state.json')


if __name__ == "__main__":
    asyncio.run(run_meta_learner_demo())
