#!/usr/bin/env python3
"""
üöÄ FIXED GRU Trading Model - –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ç–æ—Ä–≥–æ–≤–ª–µ
=========================================================

–ö–õ–Æ–ß–ï–í–´–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø:
1. ‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –ü–†–û–¶–ï–ù–¢–ù–´–ï –ò–ó–ú–ï–ù–ï–ù–ò–Ø, –∞ –Ω–µ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ —Ü–µ–Ω—ã
2. ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω Attention –º–µ—Ö–∞–Ω–∏–∑–º –¥–ª—è –ª—É—á—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤  
3. ‚úÖ Layer Normalization –≤–º–µ—Å—Ç–æ Batch Normalization
4. ‚úÖ –£–º–µ–Ω—å—à–µ–Ω dropout –¥–æ —Ä–∞–∑—É–º–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (0.2-0.3)
5. ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω—ã –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏ (—á–∞—Å, –¥–µ–Ω—å –Ω–µ–¥–µ–ª–∏, –º–µ—Å—è—Ü)
6. ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è loss —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç—Ä–µ–π–¥–∏–Ω–≥–∞
7. ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω—ã –º–µ—Ç—Ä–∏–∫–∏ Sharpe Ratio –∏ Profit Factor
8. ‚úÖ Multi-head attention –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä–∞–∑–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–∞—Å—à—Ç–∞–±–æ–≤
"""

import asyncio
import logging
import sys
import time
from pathlib import Path
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Optional, Tuple
import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –∫–æ—Ä–Ω—é –ø—Ä–æ–µ–∫—Ç–∞
sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import Dataset, DataLoader
    import torch.nn.functional as F
    from sklearn.preprocessing import RobustScaler, StandardScaler
except ImportError as e:
    print(f"‚ùå Import error: {e}")
    print("Install: pip install torch torchvision scikit-learn")
    sys.exit(1)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


# ==========================================
# üéÆ GPU CONFIGURATION
# ==========================================

def configure_gpu():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ PyTorch –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU."""
    logger.info("üéÆ Configuring GPU...")
    
    if torch.cuda.is_available():
        device = torch.device('cuda')
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        
        logger.info(f"‚úÖ GPU: {gpu_name} ({gpu_memory:.1f} GB)")
        logger.info(f"   CUDA: {torch.version.cuda}")
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.enabled = True
        
        return device
    else:
        logger.info("üìä Using CPU")
        return torch.device('cpu')


# ==========================================
# üß† MULTI-HEAD ATTENTION MODULE
# ==========================================

class MultiHeadAttention(nn.Module):
    """Multi-head attention –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
    
    def __init__(self, hidden_size: int, num_heads: int = 4, dropout: float = 0.2):
        super().__init__()
        assert hidden_size % num_heads == 0
        
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        
        self.query = nn.Linear(hidden_size, hidden_size)
        self.key = nn.Linear(hidden_size, hidden_size)
        self.value = nn.Linear(hidden_size, hidden_size)
        
        self.fc_out = nn.Linear(hidden_size, hidden_size)
        self.dropout = nn.Dropout(dropout)
        self.layer_norm = nn.LayerNorm(hidden_size)
        
    def forward(self, x):
        batch_size, seq_length, _ = x.shape
        
        # –í—ã—á–∏—Å–ª—è–µ–º Q, K, V
        Q = self.query(x)
        K = self.key(x)
        V = self.value(x)
        
        # Reshape –¥–ª—è multi-head
        Q = Q.view(batch_size, seq_length, self.num_heads, self.head_dim).permute(0, 2, 1, 3)
        K = K.view(batch_size, seq_length, self.num_heads, self.head_dim).permute(0, 2, 1, 3)
        V = V.view(batch_size, seq_length, self.num_heads, self.head_dim).permute(0, 2, 1, 3)
        
        # Attention scores
        scores = torch.matmul(Q, K.permute(0, 1, 3, 2)) / (self.head_dim ** 0.5)
        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        
        # Apply attention to values
        context = torch.matmul(attention_weights, V)
        context = context.permute(0, 2, 1, 3).contiguous()
        context = context.view(batch_size, seq_length, self.hidden_size)
        
        # Final linear layer
        output = self.fc_out(context)
        output = self.dropout(output)
        
        # Residual connection –∏ layer norm
        return self.layer_norm(output + x)


# ==========================================
# üß† FIXED GRU MODEL WITH ATTENTION
# ==========================================

class TradingGRUModel(nn.Module):
    """
    –£–ª—É—á—à–µ–Ω–Ω–∞—è GRU –º–æ–¥–µ–ª—å –¥–ª—è —Ç—Ä–µ–π–¥–∏–Ω–≥–∞:
    - Multi-head Attention –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
    - Layer Normalization –≤–º–µ—Å—Ç–æ Batch Norm
    - –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π
    - –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π dropout
    """
    
    def __init__(self, input_features: int, sequence_length: int, 
                 hidden_size: int = 128, num_heads: int = 4):
        super().__init__()
        
        self.input_features = input_features
        self.sequence_length = sequence_length
        self.hidden_size = hidden_size
        
        # Input projection
        self.input_projection = nn.Linear(input_features, hidden_size)
        self.layer_norm_input = nn.LayerNorm(hidden_size)
        
        # GRU layers —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ —Ä–∞–∑–º–µ—Ä–∞–º–∏
        self.gru1 = nn.GRU(
            input_size=hidden_size,
            hidden_size=hidden_size,
            num_layers=2,
            batch_first=True,
            dropout=0.2,
            bidirectional=True  # Bidirectional –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        )
        
        # –ü–æ—Å–ª–µ bidirectional GRU —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —É–¥–≤–∞–∏–≤–∞–µ—Ç—Å—è
        self.layer_norm1 = nn.LayerNorm(hidden_size * 2)
        
        # Multi-head attention
        self.attention = MultiHeadAttention(
            hidden_size * 2, 
            num_heads=num_heads,
            dropout=0.2
        )
        
        # Projection –æ–±—Ä–∞—Ç–Ω–æ –∫ hidden_size
        self.projection = nn.Linear(hidden_size * 2, hidden_size)
        self.layer_norm2 = nn.LayerNorm(hidden_size)
        
        # GRU layer 2
        self.gru2 = nn.GRU(
            input_size=hidden_size,
            hidden_size=hidden_size // 2,
            num_layers=1,
            batch_first=True,
            dropout=0.0
        )
        
        self.layer_norm3 = nn.LayerNorm(hidden_size // 2)
        
        # Dropout —Å–ª–æ–∏ —Å —Ä–∞–∑—É–º–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
        self.dropout1 = nn.Dropout(0.2)
        self.dropout2 = nn.Dropout(0.3)
        self.dropout3 = nn.Dropout(0.2)
        
        # Output layers
        self.fc1 = nn.Linear(hidden_size // 2, 32)
        self.fc2 = nn.Linear(32, 16)
        self.fc3 = nn.Linear(16, 3)  # 3 –≤—ã—Ö–æ–¥–∞: –≤–≤–µ—Ä—Ö, –≤–Ω–∏–∑, –±–æ–∫–æ–≤–∏–∫
        
        self.relu = nn.ReLU()
        self.leaky_relu = nn.LeakyReLU(0.01)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        self._initialize_weights()
        
    def _initialize_weights(self):
        """–ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤"""
        for name, param in self.named_parameters():
            if 'weight' in name:
                if 'gru' in name:
                    nn.init.orthogonal_(param)  # –õ—É—á—à–µ –¥–ª—è RNN
                elif 'fc' in name or 'projection' in name:
                    nn.init.kaiming_normal_(param, mode='fan_out', nonlinearity='relu')
            elif 'bias' in name:
                nn.init.constant_(param, 0.0)
                
    def forward(self, x):
        # Input projection
        x = self.input_projection(x)
        x = self.layer_norm_input(x)
        x = self.dropout1(x)
        
        # GRU Layer 1 (bidirectional)
        out, _ = self.gru1(x)
        out = self.layer_norm1(out)
        out = self.dropout1(out)
        
        # Multi-head attention
        out = self.attention(out)
        
        # Projection back
        out = self.projection(out)
        out = self.layer_norm2(out)
        out = self.dropout2(out)
        
        # GRU Layer 2
        out, _ = self.gru2(out)
        out = self.layer_norm3(out)
        out = self.dropout2(out)
        
        # –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–π timestep
        out = out[:, -1, :]
        
        # Output layers
        out = self.fc1(out)
        out = self.leaky_relu(out)
        out = self.dropout3(out)
        
        out = self.fc2(out)
        out = self.leaky_relu(out)
        
        out = self.fc3(out)
        
        return out  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º logits –¥–ª—è 3 –∫–ª–∞—Å—Å–æ–≤


# ==========================================
# üìä CUSTOM TRADING LOSS
# ==========================================

class TradingLoss(nn.Module):
    """
    –ö–∞—Å—Ç–æ–º–Ω–∞—è loss —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç—Ä–µ–π–¥–∏–Ω–≥–∞:
    - –£—á–∏—Ç—ã–≤–∞–µ—Ç –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–≤–∏–∂–µ–Ω–∏—è
    - –®—Ç—Ä–∞—Ñ—É–µ—Ç –∑–∞ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –±–æ–ª—å—à–µ
    - –£—á–∏—Ç—ã–≤–∞–µ—Ç –≤–µ–ª–∏—á–∏–Ω—É –¥–≤–∏–∂–µ–Ω–∏—è
    """
    
    def __init__(self, direction_weight: float = 2.0):
        super().__init__()
        self.direction_weight = direction_weight
        self.ce_loss = nn.CrossEntropyLoss()
        
    def forward(self, predictions, targets, price_changes):
        """
        predictions: –ª–æ–≥–∏—Ç—ã –¥–ª—è 3 –∫–ª–∞—Å—Å–æ–≤ (–≤–≤–µ—Ä—Ö, –≤–Ω–∏–∑, –±–æ–∫–æ–≤–∏–∫)
        targets: –∫–ª–∞—Å—Å (0=–≤–Ω–∏–∑, 1=–±–æ–∫–æ–≤–∏–∫, 2=–≤–≤–µ—Ä—Ö)
        price_changes: —Ä–µ–∞–ª—å–Ω–æ–µ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–Ω—ã
        """
        # Classification loss
        ce_loss = self.ce_loss(predictions, targets)
        
        # Direction penalty - —à—Ç—Ä–∞—Ñ—É–µ–º —Å–∏–ª—å–Ω–µ–µ –∑–∞ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
        pred_probs = F.softmax(predictions, dim=1)
        pred_direction = torch.argmax(pred_probs, dim=1).float() - 1  # -1, 0, 1
        true_direction = targets.float() - 1  # -1, 0, 1
        
        direction_error = torch.abs(pred_direction - true_direction)
        direction_loss = torch.mean(direction_error * torch.abs(price_changes))
        
        # –û–±—â–∞—è loss
        total_loss = ce_loss + self.direction_weight * direction_loss
        
        return total_loss


# ==========================================
# üìä DATA PREPARATION WITH PROPER FEATURES
# ==========================================

def add_temporal_features(df: pd.DataFrame) -> pd.DataFrame:
    """–î–æ–±–∞–≤–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏"""
    df = df.copy()
    
    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏
    df['hour'] = pd.to_datetime(df.index).hour
    df['day_of_week'] = pd.to_datetime(df.index).dayofweek
    df['day_of_month'] = pd.to_datetime(df.index).day
    df['month'] = pd.to_datetime(df.index).month
    
    # –¶–∏–∫–ª–∏—á–µ—Å–∫–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏
    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
    df['dow_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)
    df['dow_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)
    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)
    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)
    
    # –£–¥–∞–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    df.drop(['hour', 'day_of_week', 'day_of_month', 'month'], axis=1, inplace=True)
    
    return df


def add_price_features(df: pd.DataFrame) -> pd.DataFrame:
    """–î–æ–±–∞–≤–∏—Ç—å —Ñ–∏—á–∏ –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞ —Ü–µ–Ω–∞—Ö"""
    df = df.copy()
    
    # –ü—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
    for col in ['open', 'high', 'low', 'close', 'volume']:
        df[f'{col}_pct'] = df[col].pct_change()
        df[f'{col}_pct_ma5'] = df[f'{col}_pct'].rolling(5).mean()
        df[f'{col}_pct_std5'] = df[f'{col}_pct'].rolling(5).std()
    
    # Log returns
    df['log_return'] = np.log(df['close'] / df['close'].shift(1))
    df['log_return_ma5'] = df['log_return'].rolling(5).mean()
    df['log_return_std5'] = df['log_return'].rolling(5).std()
    
    # Price momentum
    for period in [5, 10, 20]:
        df[f'momentum_{period}'] = df['close'].pct_change(period)
    
    # High-Low spread
    df['hl_spread'] = (df['high'] - df['low']) / df['close']
    df['oc_spread'] = (df['close'] - df['open']) / df['open']
    
    # Volatility
    df['volatility_5'] = df['log_return'].rolling(5).std() * np.sqrt(252)
    df['volatility_20'] = df['log_return'].rolling(20).std() * np.sqrt(252)
    
    # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN
    df.fillna(method='ffill', inplace=True)
    df.fillna(0, inplace=True)
    
    return df


def prepare_trading_sequences(
    df: pd.DataFrame,
    feature_columns: List[str],
    sequence_length: int = 60,
    train_ratio: float = 0.7,
    val_ratio: float = 0.15,
    price_change_threshold: float = 0.001  # 0.1% –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –±–æ–∫–æ–≤–∏–∫–∞
) -> Tuple:
    """
    –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Ç—Ä–µ–π–¥–∏–Ω–≥–∞.
    
    –ö–õ–Æ–ß–ï–í–û–ï –û–¢–õ–ò–ß–ò–ï: –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –ö–õ–ê–°–° –¥–≤–∏–∂–µ–Ω–∏—è —Ü–µ–Ω—ã, –∞ –Ω–µ —Å–∞–º—É —Ü–µ–Ω—É!
    """
    logger.info(f"üì¶ Preparing trading sequences...")
    logger.info(f"   Sequence length: {sequence_length}")
    logger.info(f"   Features: {len(feature_columns)}")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏
    df = add_temporal_features(df)
    df = add_price_features(df)
    
    # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∏—á–µ–π
    temporal_features = ['hour_sin', 'hour_cos', 'dow_sin', 'dow_cos', 'month_sin', 'month_cos']
    price_features = [col for col in df.columns if '_pct' in col or 'momentum' in col or 
                      'spread' in col or 'volatility' in col or 'log_return' in col]
    
    all_features = feature_columns + temporal_features + price_features
    all_features = [f for f in all_features if f in df.columns]
    
    logger.info(f"   Total features with temporal: {len(all_features)}")
    
    # –í—ã—á–∏—Å–ª—è–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é - –ü–†–û–¶–ï–ù–¢–ù–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï
    df['target_pct_change'] = df['close'].shift(-1).pct_change()
    
    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: 0 = –ø–∞–¥–µ–Ω–∏–µ, 1 = –±–æ–∫–æ–≤–∏–∫, 2 = —Ä–æ—Å—Ç
    df['target_class'] = 1  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –±–æ–∫–æ–≤–∏–∫
    df.loc[df['target_pct_change'] < -price_change_threshold, 'target_class'] = 0
    df.loc[df['target_pct_change'] > price_change_threshold, 'target_class'] = 2
    
    # –£–¥–∞–ª—è–µ–º NaN
    df.dropna(inplace=True)
    
    # –í—Ä–µ–º–µ–Ω–Ω–æ–π —Å–ø–ª–∏—Ç
    n = len(df)
    train_end = int(n * train_ratio)
    val_end = int(n * (train_ratio + val_ratio))
    
    df_train = df.iloc[:train_end].copy()
    df_val = df.iloc[train_end:val_end].copy()
    df_test = df.iloc[val_end:].copy()
    
    logger.info(f"‚úÖ Temporal split:")
    logger.info(f"   Train: {len(df_train):,} samples")
    logger.info(f"   Val:   {len(df_val):,} samples")
    logger.info(f"   Test:  {len(df_test):,} samples")
    
    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
    for name, data in [('Train', df_train), ('Val', df_val), ('Test', df_test)]:
        class_dist = data['target_class'].value_counts().sort_index()
        logger.info(f"   {name} classes: Down={class_dist.get(0, 0)}, "
                   f"Sideways={class_dist.get(1, 0)}, Up={class_dist.get(2, 0)}")
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–∏—á–µ–π (fit —Ç–æ–ª—å–∫–æ –Ω–∞ train!)
    scaler = RobustScaler()
    df_train[all_features] = scaler.fit_transform(df_train[all_features])
    df_val[all_features] = scaler.transform(df_val[all_features])
    df_test[all_features] = scaler.transform(df_test[all_features])
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
    def create_sequences(data, features):
        X, y_class, y_pct = [], [], []
        for i in range(len(data) - sequence_length):
            X.append(data[features].iloc[i:i + sequence_length].values)
            y_class.append(data['target_class'].iloc[i + sequence_length])
            y_pct.append(data['target_pct_change'].iloc[i + sequence_length])
        return np.array(X), np.array(y_class), np.array(y_pct)
    
    X_train, y_train_class, y_train_pct = create_sequences(df_train, all_features)
    X_val, y_val_class, y_val_pct = create_sequences(df_val, all_features)
    X_test, y_test_class, y_test_pct = create_sequences(df_test, all_features)
    
    logger.info(f"‚úÖ Sequences created:")
    logger.info(f"   X_train: {X_train.shape}")
    logger.info(f"   X_val: {X_val.shape}")
    logger.info(f"   X_test: {X_test.shape}")
    
    return (X_train, X_val, X_test, 
            y_train_class, y_val_class, y_test_class,
            y_train_pct, y_val_pct, y_test_pct,
            scaler, all_features)


# ==========================================
# üìä TRADING METRICS
# ==========================================

def calculate_trading_metrics(predictions, targets, price_changes):
    """
    –†–∞—Å—Å—á–∏—Ç–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ç—Ä–µ–π–¥–∏–Ω–≥–∞:
    - Accuracy
    - Precision/Recall –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
    - Sharpe Ratio
    - Profit Factor
    """
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if torch.is_tensor(predictions):
        predictions = predictions.cpu().numpy()
    if torch.is_tensor(targets):
        targets = targets.cpu().numpy()
    if torch.is_tensor(price_changes):
        price_changes = price_changes.cpu().numpy()
    
    # Accuracy
    accuracy = np.mean(predictions == targets) * 100
    
    # Per-class metrics
    metrics = {}
    for class_idx, class_name in enumerate(['Down', 'Sideways', 'Up']):
        mask = targets == class_idx
        if mask.sum() > 0:
            precision = np.mean(predictions[predictions == class_idx] == class_idx) * 100
            recall = np.mean(predictions[mask] == class_idx) * 100
            metrics[class_name] = {'precision': precision, 'recall': recall}
    
    # –°–∏–º—É–ª—è—Ü–∏—è —Ç–æ—Ä–≥–æ–≤–ª–∏
    trading_signals = predictions - 1  # -1, 0, 1
    returns = trading_signals[:-1] * price_changes[1:]  # –°–¥–≤–∏–≥ –Ω–∞ 1 –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è
    
    # Sharpe Ratio (annualized)
    if len(returns) > 0 and returns.std() > 0:
        sharpe_ratio = np.sqrt(252 * 48) * returns.mean() / returns.std()  # 48 = —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø–µ—Ä–∏–æ–¥–æ–≤ –≤ –¥–µ–Ω—å –¥–ª—è 30m
    else:
        sharpe_ratio = 0
    
    # Profit Factor
    gains = returns[returns > 0].sum()
    losses = abs(returns[returns < 0].sum())
    profit_factor = gains / losses if losses > 0 else np.inf if gains > 0 else 0
    
    # Win Rate
    winning_trades = (returns > 0).sum()
    total_trades = (trading_signals[:-1] != 0).sum()
    win_rate = winning_trades / total_trades * 100 if total_trades > 0 else 0
    
    return {
        'accuracy': accuracy,
        'class_metrics': metrics,
        'sharpe_ratio': sharpe_ratio,
        'profit_factor': profit_factor,
        'win_rate': win_rate,
        'total_return': returns.sum() * 100  # –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
    }


# ==========================================
# üéì TRAINING WITH TRADING METRICS
# ==========================================

def train_trading_model(
    model: nn.Module,
    train_loader: DataLoader,
    val_loader: DataLoader,
    device: torch.device,
    epochs: int = 50,
    initial_lr: float = 0.001,
    patience: int = 10
) -> Dict:
    """
    –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –º–µ—Ç—Ä–∏–∫–∏ —Ç—Ä–µ–π–¥–∏–Ω–≥–∞
    """
    logger.info(f"üéØ Training Trading Model...")
    logger.info(f"   Epochs: {epochs}")
    logger.info(f"   Initial LR: {initial_lr}")
    logger.info(f"   Device: {device}")
    
    # Loss –∏ optimizer
    criterion = TradingLoss(direction_weight=2.0)
    optimizer = optim.AdamW(
        model.parameters(),
        lr=initial_lr,
        weight_decay=1e-5,
        betas=(0.9, 0.999)
    )
    
    # Scheduler
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, 
        T_0=10,  # Restart every 10 epochs
        T_mult=2,
        eta_min=1e-6
    )
    
    # Early stopping
    best_sharpe = -np.inf
    patience_counter = 0
    best_model_state = None
    
    history = {
        'train_loss': [],
        'val_loss': [],
        'train_accuracy': [],
        'val_accuracy': [],
        'val_sharpe': [],
        'val_profit_factor': [],
        'learning_rates': []
    }
    
    for epoch in range(epochs):
        # ===== TRAINING =====
        model.train()
        train_losses = []
        train_predictions = []
        train_targets = []
        
        for batch_X, batch_y_class, batch_y_pct in train_loader:
            batch_X = batch_X.to(device)
            batch_y_class = batch_y_class.to(device)
            batch_y_pct = batch_y_pct.to(device)
            
            optimizer.zero_grad()
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y_class.long(), batch_y_pct)
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            train_losses.append(loss.item())
            train_predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())
            train_targets.extend(batch_y_class.cpu().numpy())
        
        # ===== VALIDATION =====
        model.eval()
        val_losses = []
        val_predictions = []
        val_targets = []
        val_price_changes = []
        
        with torch.no_grad():
            for batch_X, batch_y_class, batch_y_pct in val_loader:
                batch_X = batch_X.to(device)
                batch_y_class = batch_y_class.to(device)
                batch_y_pct = batch_y_pct.to(device)
                
                outputs = model(batch_X)
                loss = criterion(outputs, batch_y_class.long(), batch_y_pct)
                
                val_losses.append(loss.item())
                val_predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())
                val_targets.extend(batch_y_class.cpu().numpy())
                val_price_changes.extend(batch_y_pct.cpu().numpy())
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        train_metrics = calculate_trading_metrics(
            np.array(train_predictions),
            np.array(train_targets),
            np.zeros_like(train_predictions)  # –î–ª—è train –Ω–µ —Å—á–∏—Ç–∞–µ–º Sharpe
        )
        
        val_metrics = calculate_trading_metrics(
            np.array(val_predictions),
            np.array(val_targets),
            np.array(val_price_changes)
        )
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
        avg_train_loss = np.mean(train_losses)
        avg_val_loss = np.mean(val_losses)
        
        history['train_loss'].append(avg_train_loss)
        history['val_loss'].append(avg_val_loss)
        history['train_accuracy'].append(train_metrics['accuracy'])
        history['val_accuracy'].append(val_metrics['accuracy'])
        history['val_sharpe'].append(val_metrics['sharpe_ratio'])
        history['val_profit_factor'].append(val_metrics['profit_factor'])
        history['learning_rates'].append(optimizer.param_groups[0]['lr'])
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        logger.info(
            f"Epoch {epoch+1:3d}/{epochs} | "
            f"Loss: {avg_train_loss:.4f}/{avg_val_loss:.4f} | "
            f"Acc: {train_metrics['accuracy']:.1f}%/{val_metrics['accuracy']:.1f}% | "
            f"Sharpe: {val_metrics['sharpe_ratio']:.2f} | "
            f"PF: {val_metrics['profit_factor']:.2f}"
        )
        
        # Learning rate scheduler
        scheduler.step()
        
        # Early stopping –ø–æ Sharpe Ratio
        if val_metrics['sharpe_ratio'] > best_sharpe:
            best_sharpe = val_metrics['sharpe_ratio']
            best_model_state = model.state_dict().copy()
            patience_counter = 0
            logger.info(f"   üíæ New best model! Sharpe: {best_sharpe:.3f}")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                logger.info(f"   üõë Early stopping triggered!")
                model.load_state_dict(best_model_state)
                break
    
    logger.info(f"‚úÖ Training completed! Best Sharpe: {best_sharpe:.3f}")
    
    return history


# ==========================================
# üìä DATASET CLASS
# ==========================================

class TradingDataset(Dataset):
    """Dataset –¥–ª—è —Ç—Ä–µ–π–¥–∏–Ω–≥–∞ —Å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π"""
    
    def __init__(self, X, y_class, y_pct):
        self.X = torch.FloatTensor(X)
        self.y_class = torch.LongTensor(y_class)
        self.y_pct = torch.FloatTensor(y_pct)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y_class[idx], self.y_pct[idx]


# ==========================================
# üöÄ MAIN TRAINING FUNCTION
# ==========================================

async def train_fixed_gru(
    symbols: List[str] = None,
    days: int = 365,
    interval: str = "30m",
    sequence_length: int = 60,
    epochs: int = 100,
    batch_size: int = 32,
    save_path: str = "models/checkpoints/gru_trading_fixed.pt",
    use_cache: bool = True
):
    """
    –û–±—É—á–∏—Ç—å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—É—é GRU –º–æ–¥–µ–ª—å –¥–ª—è —Ç—Ä–µ–π–¥–∏–Ω–≥–∞
    """
    # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    sys.path.insert(0, str(Path(__file__).parent))
    from gru_training_pytorch import (
        BinanceDataDownloader,
        calculate_technical_indicators
    )
    
    # Default symbols
    if symbols is None:
        symbols = [
            'BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'SOLUSDT',
            'ADAUSDT', 'XRPUSDT', 'DOGEUSDT'
        ]
    
    logger.info("=" * 80)
    logger.info("üöÄ FIXED GRU Trading Model Training")
    logger.info("=" * 80)
    logger.info(f"üìã Configuration:")
    logger.info(f"   Symbols: {', '.join(symbols)}")
    logger.info(f"   Days: {days}")
    logger.info(f"   Sequence: {sequence_length}")
    logger.info(f"   Epochs: {epochs}")
    logger.info(f"   Batch size: {batch_size}")
    logger.info("=" * 80)
    
    # GPU setup
    device = configure_gpu()
    
    # ===== LOAD DATA =====
    combined_df = None
    
    if use_cache:
        try:
            sys.path.insert(0, str(Path(__file__).parent.parent))
            from scripts.download_and_cache_data import load_cached_data
            logger.info("üìÇ Loading cached data...")
            combined_df = load_cached_data(symbols, days, interval)
        except:
            logger.warning("‚ö†Ô∏è  Cache not available")
    
    if combined_df is None:
        downloader = BinanceDataDownloader()
        all_data = []
        
        for i, symbol in enumerate(symbols, 1):
            logger.info(f"üì• Downloading {symbol} ({i}/{len(symbols)})...")
            df = await downloader.download_historical_data(symbol, interval, days)
            
            if len(df) > 0:
                df = calculate_technical_indicators(df)
                # –î–æ–±–∞–≤–ª—è–µ–º symbol –∫–∞–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏—é
                df['symbol'] = symbol
                all_data.append(df)
            else:
                logger.warning(f"‚ö†Ô∏è  Skipping {symbol}")
        
        combined_df = pd.concat(all_data, ignore_index=True)
        logger.info(f"‚úÖ Combined: {len(combined_df):,} samples")
    
    # ===== FEATURE COLUMNS =====
    feature_columns = [
        'open', 'high', 'low', 'volume',
        'rsi', 'macd', 'macd_signal',
        'bb_upper', 'bb_mid', 'bb_lower',
        'sma_20', 'sma_50', 'ema_50',
        'volume_sma', 'atr',
        'volume_delta', 'obv', 'volume_ratio',
        'volume_spike', 'mfi', 'cvd', 'vwap_distance'
    ]
    
    # ===== PREPARE DATA =====
    (X_train, X_val, X_test,
     y_train_class, y_val_class, y_test_class,
     y_train_pct, y_val_pct, y_test_pct,
     scaler, all_features) = prepare_trading_sequences(
        combined_df,
        feature_columns,
        sequence_length,
        train_ratio=0.7,
        val_ratio=0.15
    )
    
    # ===== CREATE DATALOADERS =====
    train_dataset = TradingDataset(X_train, y_train_class, y_train_pct)
    val_dataset = TradingDataset(X_val, y_val_class, y_val_pct)
    test_dataset = TradingDataset(X_test, y_test_class, y_test_pct)
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,  # –î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å shuffle
        num_workers=0,
        pin_memory=True
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0,
        pin_memory=True
    )
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0,
        pin_memory=True
    )
    
    # ===== CREATE MODEL =====
    logger.info("üß† Building FIXED Trading GRU model...")
    model = TradingGRUModel(
        input_features=len(all_features),
        sequence_length=sequence_length,
        hidden_size=128,
        num_heads=4
    ).to(device)
    
    total_params = sum(p.numel() for p in model.parameters())
    logger.info(f"‚úÖ Model parameters: {total_params:,}")
    
    # ===== TRAIN =====
    history = train_trading_model(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        device=device,
        epochs=epochs,
        initial_lr=0.001,
        patience=15
    )
    
    # ===== FINAL EVALUATION =====
    logger.info("=" * 80)
    logger.info("üìä Final Evaluation on Test Set")
    logger.info("=" * 80)
    
    model.eval()
    test_predictions = []
    test_targets = []
    test_price_changes = []
    
    with torch.no_grad():
        for batch_X, batch_y_class, batch_y_pct in test_loader:
            batch_X = batch_X.to(device)
            outputs = model(batch_X)
            test_predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())
            test_targets.extend(batch_y_class.numpy())
            test_price_changes.extend(batch_y_pct.numpy())
    
    # –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    test_metrics = calculate_trading_metrics(
        np.array(test_predictions),
        np.array(test_targets),
        np.array(test_price_changes)
    )
    
    logger.info(f"üìä Test Set Metrics:")
    logger.info(f"   Accuracy: {test_metrics['accuracy']:.2f}%")
    logger.info(f"   Sharpe Ratio: {test_metrics['sharpe_ratio']:.3f}")
    logger.info(f"   Profit Factor: {test_metrics['profit_factor']:.2f}")
    logger.info(f"   Win Rate: {test_metrics['win_rate']:.2f}%")
    logger.info(f"   Total Return: {test_metrics['total_return']:.2f}%")
    
    # Class-specific metrics
    for class_name, metrics in test_metrics['class_metrics'].items():
        logger.info(f"   {class_name}: Precision={metrics['precision']:.1f}%, "
                   f"Recall={metrics['recall']:.1f}%")
    
    # ===== SAVE MODEL =====
    save_dir = Path(save_path).parent
    save_dir.mkdir(parents=True, exist_ok=True)
    
    torch.save({
        'model_state_dict': model.state_dict(),
        'model_config': {
            'input_features': len(all_features),
            'sequence_length': sequence_length,
            'feature_columns': all_features,
            'hidden_size': 128,
            'num_heads': 4
        },
        'scaler': scaler,
        'training_history': history,
        'final_metrics': test_metrics,
        'model_type': 'classification',  # –í–∞–∂–Ω–æ –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
        'num_classes': 3
    }, save_path)
    
    logger.info(f"‚úÖ Model saved: {save_path}")
    logger.info(f"   Size: {Path(save_path).stat().st_size / 1024 / 1024:.1f} MB")
    
    logger.info("=" * 80)
    logger.info("üéâ FIXED TRAINING COMPLETED!")
    logger.info("=" * 80)
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç—Ä–∏–∫
    if test_metrics['sharpe_ratio'] < 0.5:
        logger.warning("‚ö†Ô∏è  Low Sharpe Ratio. Recommendations:")
        logger.warning("   - Increase training data (more symbols/days)")
        logger.warning("   - Tune hyperparameters")
        logger.warning("   - Add more features (order flow, sentiment)")
    elif test_metrics['sharpe_ratio'] < 1.0:
        logger.info("üìà Decent Sharpe Ratio. Can be improved:")
        logger.info("   - Fine-tune on specific market conditions")
        logger.info("   - Add ensemble models")
    else:
        logger.info("üöÄ Excellent Sharpe Ratio! Ready for trading!")
    
    logger.info("=" * 80)


# ==========================================
# üöÄ MAIN
# ==========================================

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Train FIXED GRU Trading Model")
    parser.add_argument('--days', type=int, default=365, help='Days of data')
    parser.add_argument('--interval', type=str, default='30m', 
                       help='Timeframe: 1m, 5m, 15m, 30m, 1h, 4h')
    parser.add_argument('--sequence-length', type=int, default=60, 
                       help='Sequence length')
    parser.add_argument('--epochs', type=int, default=100, help='Max epochs')
    parser.add_argument('--batch-size', type=int, default=32, help='Batch size')
    parser.add_argument('--use-cache', action='store_true', help='Use cached data')
    parser.add_argument('--symbols', type=str, nargs='+', help='Trading symbols')
    
    args = parser.parse_args()
    
    asyncio.run(train_fixed_gru(
        symbols=args.symbols,
        days=args.days,
        interval=args.interval,
        sequence_length=args.sequence_length,
        epochs=args.epochs,
        batch_size=args.batch_size,
        use_cache=args.use_cache
    ))
